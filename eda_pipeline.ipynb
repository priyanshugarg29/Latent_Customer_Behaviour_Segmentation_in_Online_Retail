{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b3645e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# EDA Pipeline for RetailRocket Clickstream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141c951-c9d5-4ead-8e27-317f441592e9",
   "metadata": {},
   "source": [
    "## 1) Imports & configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "588320e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1) Imports & environment\n",
    "\n",
    "import os, json, math, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict, deque\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c29076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2) CONFIGURATION — kindly adjust as needed (all parameters centralised here)\n",
    "from pathlib import Path\n",
    "\n",
    "#  Paths\n",
    "DATA_DIR            = Path(\"C:/Users/Admin/Documents/WBS/Dissertation/Submission Related files/data\")  # folder that holds the CSVs\n",
    "EVENTS_CSV          = DATA_DIR / \"events.csv\"      # expected columns: session_id*, visitorid*, itemid, timestamp, event\n",
    "ITEMS1_CSV          = DATA_DIR / \"item_properties_part1.csv\"\n",
    "ITEMS2_CSV          = DATA_DIR / \"item_properties_part2.csv\"   \n",
    "# Core switches\n",
    "HAS_SESSION_ID      = False     # if False, sessionize via visitorid and SESSION_GAP_SEC\n",
    "SESSION_GAP_SEC     = 30*60    # 30 minutes inactivity defines a new session when HAS_SESSION_ID=False\n",
    "TS_DIV              = 1000        # keep original UNIX timestamp unit (1 if already seconds; 1000 if ms). We retain raw UNIX as per dissertation rationale.\n",
    "EVENT_TYPES         = [\"view\", \"addtocart\", \"transaction\"]   # expected events (others will be grouped under \"other\")\n",
    "\n",
    "# Sampling & performance\n",
    "USE_STRATIFIED_SAMPLING = False    \n",
    "SAMPLE_MAX_SESSIONS     = 200_000  # cap for heavy operations (hour/day patterns, n-grams)\n",
    "RANDOM_SEED             = 42\n",
    "\n",
    "# Plots & outputs\n",
    "OUT_DIR             = Path(\"C:/Users/Admin/Documents/WBS/Dissertation/Submission Related files/Notebooks/Modelling/EDA_outputs\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR             = OUT_DIR / \"figs\"; FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAB_DIR             = OUT_DIR / \"tables\"; TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR         = OUT_DIR / \"reports\"; REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAVE_DPI            = 150\n",
    "FIG_EXT             = \"png\"           # png recommended for dashboards; use \"svg\" for vector\n",
    "STYLE_USE_SEABORN   = False           # keep False for portability; can switch to True if seaborn is installed\n",
    "PANDAS_FLOAT_FMT    = \"{:,.4f}\".format\n",
    "\n",
    "# Feature thresholds\n",
    "TOP_N               = 20              # for top products/categories/3-grams\n",
    "OUTLIER_EPM_THRESH  = 120             # events per minute threshold indicating possible automation/ bot or web scrapping\n",
    "OUTLIER_DUR_PCTL    = 99.5            # sessions above this duration percentile are flagged\n",
    "\n",
    "# Temporal granularity\n",
    "HOUR_OF_DAY         = list(range(24))\n",
    "DAYS_MAP            = {0:\"Mon\",1:\"Tue\",2:\"Wed\",3:\"Thu\",4:\"Fri\",5:\"Sat\",6:\"Sun\"}\n",
    "\n",
    "# Exports\n",
    "EXPORT_SESSION_FEATURES = True\n",
    "SESSION_FEATURES_CSV    = OUT_DIR / \"eda\" / \"session_features_base.csv\"\n",
    "\n",
    "# Behavioural funnels\n",
    "FUNNEL_ORDER        = [\"view\", \"addtocart\", \"transaction\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3108132-cffb-491a-b015-b4780d8bd853",
   "metadata": {},
   "source": [
    "## 2) Defining utilities to support pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8fec976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Utilities — tic/toc, saving, safe plotting, sampling\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime\n",
    "\n",
    "_TIC_STACK = []\n",
    "\n",
    "def tic(msg=None):\n",
    "    \"\"\"Start a timer; optionally print a message.\"\"\"\n",
    "    if msg:\n",
    "        print(f\"[tic] {msg}\")\n",
    "    _TIC_STACK.append(time.time())\n",
    "\n",
    "def toc(msg=None):\n",
    "    \"\"\"End the most recent timer and print elapsed time.\"\"\"\n",
    "    if not _TIC_STACK:\n",
    "        print(\"[toc] Warning: called without matching tic()\")\n",
    "        return 0.0\n",
    "    elapsed = time.time() - _TIC_STACK.pop()\n",
    "    if msg:\n",
    "        print(f\"[toc] {msg}: {elapsed:,.2f}s\")\n",
    "    else:\n",
    "        print(f\"[toc] {elapsed:,.2f}s\")\n",
    "    return elapsed\n",
    "\n",
    "def ensure_dir(path: Path):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_table(df: pd.DataFrame, path: Path):\n",
    "    ensure_dir(path)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"[saved] table -> {path} ({len(df):,} rows)\")\n",
    "\n",
    "def save_fig(fig, name: str):\n",
    "    filename = FIG_DIR / f\"{name}.{FIG_EXT}\"\n",
    "    ensure_dir(filename)\n",
    "    fig.savefig(filename, dpi=SAVE_DPI, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"[saved] figure -> {filename}\")\n",
    "\n",
    "def maybe_sample_sessions(df: pd.DataFrame, session_col=\"session_id\"):\n",
    "    \"\"\"Return df sampled by sessions if enabled and needed.\"\"\"\n",
    "    if not USE_STRATIFIED_SAMPLING:\n",
    "        return df\n",
    "    if \"session_id\" not in df.columns:\n",
    "        return df\n",
    "    sess_ids = df[\"session_id\"].unique()\n",
    "    if len(sess_ids) <= SAMPLE_MAX_SESSIONS:\n",
    "        return df\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    keep = rng.choice(sess_ids, size=SAMPLE_MAX_SESSIONS, replace=False)\n",
    "    out = df[df[\"session_id\"].isin(keep)].copy()\n",
    "    print(f\"[sample] sessions: {len(sess_ids):,} -> {len(keep):,}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ad4767-d77e-4ff3-a3f0-d03336e0a134",
   "metadata": {},
   "source": [
    "## 3) Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78bb70bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tic] Reading events from C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\data\\events.csv\n",
      "[toc] Loaded events: 2.33s\n",
      "[tic] Reading items metadata from C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\data\\item_properties_part1.csv\n",
      "[toc] Reading items metadata: 1.90s\n",
      "[tic] Reading items metadata from C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\data\\item_properties_part2.csv\n",
      "[toc] Reading items metadata: 5.08s\n",
      "[tic] Sessionising by visitorid + inactivity gap\n",
      "[toc] Sessionised: 2.58s\n",
      "[tic] Computing dataset overview\n",
      "Rows=2,756,101 | Sessions=1,761,660 | Users=1407580 | Items=235,061\n",
      "Date range: 2015-05-03 03:00:04 -> 2015-09-18 02:59:47\n",
      "[saved] table -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\tables\\dataset_overview.csv (1 rows)\n",
      "[toc] Overview ready: 0.23s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22786998748779297"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Data Loading — schema handling, timestamp normalization\n",
    "\n",
    "REQUIRED_COLS_ANY = {\"itemid\", \"timestamp\", \"event\"}\n",
    "OPTIONAL_COLS     = {\"session_id\", \"visitorid\"}\n",
    "\n",
    "def read_events(path: Path) -> pd.DataFrame:\n",
    "    tic(f\"Reading events from {path}\")\n",
    "    df = pd.read_csv(path, usecols=None)\n",
    "    # column name normalisation (lowercase)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    # Validating plausible column names\n",
    "    colmap = {}\n",
    "    # Mapping timestamp variants\n",
    "    for cand in [\"timestamp\", \"ts\", \"time\", \"eventtime\", \"event_time\"]:\n",
    "        if cand in df.columns:\n",
    "            colmap[\"timestamp\"] = cand; break\n",
    "    # Mapping event variants\n",
    "    for cand in [\"event\", \"action\", \"type\"]:\n",
    "        if cand in df.columns:\n",
    "            colmap[\"event\"] = cand; break\n",
    "    # Mapping itemid variants\n",
    "    for cand in [\"itemid\", \"item_id\", \"sku\", \"productid\", \"product_id\"]:\n",
    "        if cand in df.columns:\n",
    "            colmap[\"itemid\"] = cand; break\n",
    "    # Mapping session/visitor variants\n",
    "    for cand in [\"session_id\", \"sessionid\", \"sid\"]:\n",
    "        if cand in df.columns:\n",
    "            colmap[\"session_id\"] = cand; break\n",
    "    for cand in [\"visitorid\", \"user_id\", \"userid\", \"uid\"]:\n",
    "        if cand in df.columns:\n",
    "            colmap[\"visitorid\"] = cand; break\n",
    "\n",
    "    missing = [k for k in [\"timestamp\",\"event\",\"itemid\"] if k not in colmap]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns: {missing}. Found columns = {list(df.columns)}\")\n",
    "    \n",
    "    # Renaming in place to canonical names\n",
    "    df = df.rename(columns={v:k for k,v in colmap.items()})\n",
    "    # Retaining only the canonical columns\n",
    "    keep_cols = [\"timestamp\",\"event\",\"itemid\"] + [c for c in [\"session_id\",\"visitorid\"] if c in df.columns]\n",
    "    df = df[keep_cols].copy()\n",
    "\n",
    "    # Timestamp normalization, we retain UNIX epohs\n",
    "    if TS_DIV != 1:\n",
    "        df[\"timestamp\"] = (df[\"timestamp\"] // TS_DIV).astype(\"int64\")\n",
    "    else:\n",
    "        df[\"timestamp\"] = df[\"timestamp\"].astype(\"int64\")\n",
    "\n",
    "    # Event canonicalisation\n",
    "    df[\"event\"] = df[\"event\"].astype(\"string\").str.lower().str.strip()\n",
    "    df.loc[~df[\"event\"].isin(EVENT_TYPES), \"event\"] = \"other\"\n",
    "\n",
    "    # Adding datetime\n",
    "    df[\"dt\"] = pd.to_datetime(df[\"timestamp\"], unit=\"s\")\n",
    "\n",
    "    toc(\"Loaded events\")\n",
    "    return df\n",
    "\n",
    "# pulling events data\n",
    "events = read_events(EVENTS_CSV)\n",
    "\n",
    "def maybe_read_items(path):\n",
    "    if not path:\n",
    "        return None\n",
    "    tic(f\"Reading items metadata from {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    # normalise column names\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    # standardise a category column if present under common aliases\n",
    "    if \"category\" not in df.columns:\n",
    "        for cand in (\"category_id\", \"cat\", \"categorycode\", \"category_code\"):\n",
    "            if cand in df.columns:\n",
    "                df = df.rename(columns={cand: \"category\"})\n",
    "                break\n",
    "\n",
    "    toc(\"Reading items metadata\")\n",
    "    return df\n",
    "\n",
    "# pulling items files \n",
    "items1 = maybe_read_items(ITEMS1_CSV)   # keep your variable names as-is\n",
    "items2 = maybe_read_items(ITEMS2_CSV)  \n",
    "\n",
    "# Combine items table\n",
    "items = None\n",
    "for _df in (items1, items2):\n",
    "    if _df is None:\n",
    "        continue\n",
    "    items = _df if items is None else pd.concat([items, _df], ignore_index=True)\n",
    "\n",
    "if items is not None:\n",
    "    # optional: drop duplicate item ids\n",
    "    # if key: items = items.drop_duplicates(subset=[key])\n",
    "    pass\n",
    "else:\n",
    "    # No items metadata available\n",
    "    items = None\n",
    "\n",
    "\n",
    "\n",
    "# Sessionisation if needed\n",
    "if not HAS_SESSION_ID:\n",
    "    tic(\"Sessionising by visitorid + inactivity gap\")\n",
    "    events = events.sort_values([\"visitorid\",\"timestamp\"]).reset_index(drop=True)\n",
    "    # New session when gap > SESSION_GAP_SEC or visitor changes\n",
    "    gap = events[\"timestamp\"].diff().fillna(0)\n",
    "    new_user = events[\"visitorid\"].ne(events[\"visitorid\"].shift(1))\n",
    "    new_sess = (gap > SESSION_GAP_SEC) | (new_user)\n",
    "    events[\"session_id\"] = new_sess.cumsum().astype(\"int64\")\n",
    "    toc(\"Sessionised\")\n",
    "\n",
    "if \"session_id\" not in events.columns:\n",
    "    raise KeyError(\"session_id is required (either provided or created via sessionisation).\")\n",
    "\n",
    "# Basic dataset facts\n",
    "tic(\"Computing dataset overview\")\n",
    "n_rows = len(events)\n",
    "n_sessions = events[\"session_id\"].nunique()\n",
    "n_users = events[\"visitorid\"].nunique() if \"visitorid\" in events.columns else np.nan\n",
    "n_items = events[\"itemid\"].nunique()\n",
    "t0, t1 = events[\"dt\"].min(), events[\"dt\"].max()\n",
    "print(f\"Rows={n_rows:,} | Sessions={n_sessions:,} | Users={n_users} | Items={n_items:,}\")\n",
    "print(f\"Date range: {t0} -> {t1}\")\n",
    "\n",
    "overview = pd.DataFrame([{\n",
    "    \"rows\": n_rows,\n",
    "    \"sessions\": n_sessions,\n",
    "    \"users\": n_users,\n",
    "    \"items\": n_items,\n",
    "    \"date_start\": t0,\n",
    "    \"date_end\": t1\n",
    "}])\n",
    "save_table(overview, TAB_DIR / \"dataset_overview.csv\")\n",
    "toc(\"Overview ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d9c552-16cc-4c4c-b8a9-63ab1dc55ab4",
   "metadata": {},
   "source": [
    "## 4) Event distributions & proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6676bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tic] Event distribution\n",
      "[saved] table -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\tables\\event_distribution.csv (3 rows)\n",
      "[saved] figure -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\figs\\eda_event_distribution_all.png\n",
      "[toc] Event distribution ready: 0.99s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9925820827484131"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Event distributions & proportions\n",
    "\n",
    "tic(\"Event distribution\")\n",
    "evt_counts = events[\"event\"].value_counts().rename_axis(\"event\").reset_index(name=\"count\")\n",
    "evt_counts[\"prop\"] = evt_counts[\"count\"] / evt_counts[\"count\"].sum()\n",
    "save_table(evt_counts, TAB_DIR / \"event_distribution.csv\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.bar(evt_counts[\"event\"], evt_counts[\"count\"])\n",
    "ax.set_xlabel(\"Event\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Event Distribution (All)\")\n",
    "for i, v in enumerate(evt_counts[\"count\"]):\n",
    "    ax.text(i, v, f\"{int(v):,}\", ha=\"center\", va=\"bottom\", fontsize=8, rotation=0)\n",
    "save_fig(fig, \"eda_event_distribution_all\")\n",
    "toc(\"Event distribution ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c54488-f48e-40b8-8941-2fbc48730de7",
   "metadata": {},
   "source": [
    "## 5) Temporal patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27140a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tic] Hour-of-day pattern\n",
      "[saved] table -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\tables\\temporal_hour_event_counts.csv (72 rows)\n",
      "[saved] figure -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\figs\\eda_hourly_event_volume.png\n",
      "[toc] Hour-of-day ready: 2.53s\n",
      "[tic] Day-of-week pattern\n",
      "[saved] table -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\tables\\temporal_dow_event_counts.csv (21 rows)\n",
      "[saved] figure -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\figs\\eda_weekly_event_volume.png\n",
      "[toc] Day-of-week ready: 2.25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.247720956802368"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) Temporal patterns (hour-of-day, day-of-week)\n",
    "\n",
    "df_time = maybe_sample_sessions(events)\n",
    "\n",
    "# Hour-of-day temporal pattern\n",
    "tic(\"Hour-of-day pattern\")\n",
    "hod = df_time.copy()\n",
    "hod[\"hour\"] = hod[\"dt\"].dt.hour\n",
    "hod_counts = hod.groupby([\"hour\",\"event\"]).size().reset_index(name=\"count\")\n",
    "save_table(hod_counts, TAB_DIR / \"temporal_hour_event_counts.csv\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "for e in EVENT_TYPES + ([\"other\"] if \"other\" in hod[\"event\"].unique() else []):\n",
    "    sub = hod_counts[hod_counts[\"event\"]==e]\n",
    "    if not len(sub):\n",
    "        continue\n",
    "    ax.plot(sub[\"hour\"], sub[\"count\"], marker=\"o\", label=e)\n",
    "ax.set_xticks(HOUR_OF_DAY)\n",
    "ax.set_xlabel(\"Hour of Day\")\n",
    "ax.set_ylabel(\"Events\")\n",
    "ax.set_title(\"Hourly Event Volume\")\n",
    "ax.legend()\n",
    "save_fig(fig, \"eda_hourly_event_volume\")\n",
    "toc(\"Hour-of-day ready\")\n",
    "\n",
    "# Day of week temporal pattern \n",
    "tic(\"Day-of-week pattern\")\n",
    "dow = df_time.copy()\n",
    "dow[\"dow\"] = dow[\"dt\"].dt.dayofweek\n",
    "dow_counts = dow.groupby([\"dow\",\"event\"]).size().reset_index(name=\"count\")\n",
    "dow_counts[\"dow_label\"] = dow_counts[\"dow\"].map(DAYS_MAP)\n",
    "save_table(dow_counts, TAB_DIR / \"temporal_dow_event_counts.csv\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "for e in EVENT_TYPES + ([\"other\"] if \"other\" in dow[\"event\"].unique() else []):\n",
    "    sub = dow_counts[dow_counts[\"event\"]==e]\n",
    "    if not len(sub):\n",
    "        continue\n",
    "    ax.plot(sub[\"dow_label\"], sub[\"count\"], marker=\"o\", label=e)\n",
    "ax.set_xlabel(\"Day of Week\")\n",
    "ax.set_ylabel(\"Events\")\n",
    "ax.set_title(\"Weekly Event Volume\")\n",
    "ax.legend()\n",
    "save_fig(fig, \"eda_weekly_event_volume\")\n",
    "toc(\"Day-of-week ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22d2a1-9390-4b5e-8d76-c3575ce96f4f",
   "metadata": {},
   "source": [
    "## 6) Session-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cc94340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tic] Computing session-level features\n",
      "[saved] table -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\eda\\session_features_base.csv (1,761,660 rows)\n",
      "[saved] figure -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\figs\\eda_hist_duration_log1p.png\n",
      "[saved] figure -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\figs\\eda_hist_events_log1p.png\n",
      "[saved] figure -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\figs\\eda_hist_epm_log1p.png\n",
      "[saved] figure -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\figs\\eda_hist_itemdiv_log1p.png\n",
      "[saved] table -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\tables\\session_feature_percentiles.csv (4 rows)\n",
      "[toc] Session-level features ready: 12.45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.447735071182251"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) Session-level features (duration, intensity, diversity, last-event flags)\n",
    "\n",
    "tic(\"Computing session-level features\")\n",
    "g = events.groupby(\"session_id\", sort=False)\n",
    "\n",
    "sess_first = g[\"timestamp\"].min()\n",
    "sess_last  = g[\"timestamp\"].max()\n",
    "duration_s = (sess_last - sess_first).rename(\"duration_sec\")\n",
    "n_events   = g.size().rename(\"events\")\n",
    "items_per_sess = g[\"itemid\"].nunique().rename(\"item_diversity\")\n",
    "\n",
    "# events per minute (avoid /0 by by adding a small negligable value, 1e-9 to denominator)\n",
    "events_per_min = (n_events / (duration_s/60.0 + 1e-9)).rename(\"events_per_min\")\n",
    "\n",
    "# last event type flags\n",
    "last_event = g[\"event\"].last().rename(\"last_event\")\n",
    "last_evt_flags = pd.get_dummies(last_event, prefix=\"last_evt\", dtype=int)\n",
    "\n",
    "sess_df = pd.concat([duration_s, n_events, items_per_sess, events_per_min, last_event, last_evt_flags], axis=1).reset_index()\n",
    "\n",
    "# Saving raw features\n",
    "FEATURES_DIR = OUT_DIR / \"eda\"; FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "save_table(sess_df, FEATURES_DIR / \"session_features_base.csv\")\n",
    "\n",
    "# Distributions (log-scale a they are helpful for skew)\n",
    "def _hist(series, title, xlabel, name):\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.hist(series, bins=50)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel); ax.set_ylabel(\"Frequency\")\n",
    "    save_fig(fig, name)\n",
    "\n",
    "_hist(np.log1p(sess_df[\"duration_sec\"]), \"Session Duration (log1p seconds)\", \"log1p(seconds)\", \"eda_hist_duration_log1p\")\n",
    "_hist(np.log1p(sess_df[\"events\"]), \"Session Events (log1p)\", \"log1p(events)\", \"eda_hist_events_log1p\")\n",
    "_hist(np.log1p(sess_df[\"events_per_min\"]), \"Events per Minute (log1p)\", \"log1p(epm)\", \"eda_hist_epm_log1p\")\n",
    "_hist(np.log1p(sess_df[\"item_diversity\"]), \"Item Diversity (log1p unique items)\", \"log1p(unique items)\", \"eda_hist_itemdiv_log1p\")\n",
    "\n",
    "# Percentiles summary\n",
    "pctls = sess_df[[\"duration_sec\",\"events\",\"events_per_min\",\"item_diversity\"]].quantile([0.5,0.9,0.95,0.99]).reset_index(names=[\"quantile\"])\n",
    "save_table(pctls, TAB_DIR / \"session_feature_percentiles.csv\")\n",
    "toc(\"Session-level features ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e46bc-c2b3-40b6-8c46-69711fdd6e10",
   "metadata": {},
   "source": [
    "## 7) Behavioural funnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3096f35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tic] Computing funnel\n",
      "[saved] table -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\tables\\funnel_session_level.csv (3 rows)\n",
      "[saved] figure -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\figs\\eda_funnel_session_level.png\n",
      "[toc] Funnel ready: 3.02s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.017254114151001"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7) Behavioural funnel (session-level presence of events in order)\n",
    "\n",
    "tic(\"Computing funnel\")\n",
    "# Presence flags per session\n",
    "presence = events.pivot_table(index=\"session_id\", columns=\"event\", values=\"timestamp\", aggfunc=\"count\", fill_value=0)\n",
    "for e in FUNNEL_ORDER:\n",
    "    if e not in presence.columns:\n",
    "        presence[e] = 0\n",
    "presence = presence[[e for e in FUNNEL_ORDER if e in presence.columns]].astype(int)\n",
    "\n",
    "# Simple sequential funnel to identify proportion of sessions that reach each stage\n",
    "total_sessions = presence.shape[0]\n",
    "stage_props = []\n",
    "running = total_sessions\n",
    "mask = pd.Series([True]*total_sessions, index=presence.index)\n",
    "for e in FUNNEL_ORDER:\n",
    "    step = presence.loc[mask, e] > 0\n",
    "    reached = int(step.sum())\n",
    "    stage_props.append({\"stage\": e, \"sessions_reached\": reached, \"prop_of_total\": reached/total_sessions})\n",
    "    # tighten mask for next stage: among those that reached current step\n",
    "    mask = mask & (presence[e] > 0)\n",
    "\n",
    "funnel_df = pd.DataFrame(stage_props)\n",
    "save_table(funnel_df, TAB_DIR / \"funnel_session_level.csv\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.bar(funnel_df[\"stage\"], funnel_df[\"sessions_reached\"])\n",
    "ax.set_title(\"Session-level Funnel\")\n",
    "ax.set_xlabel(\"Stage\"); ax.set_ylabel(\"Sessions Reached\")\n",
    "for i, v in enumerate(funnel_df[\"sessions_reached\"]):\n",
    "    ax.text(i, v, f\"{v:,}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "save_fig(fig, \"eda_funnel_session_level\")\n",
    "toc(\"Funnel ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1300d-2d5b-4dc6-ae42-1d5c3bdf820f",
   "metadata": {},
   "source": [
    "# 8) Sequence patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c54a247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tic] 3-gram extraction\n",
      "[saved] table -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\tables\\top_event_3grams.csv (20 rows)\n",
      "[saved] figure -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\figs\\eda_top_3grams.png\n",
      "[toc] 3-grams ready: 160.96s\n",
      "[tic] Transition matrix\n",
      "[saved] table -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\tables\\transition_matrix_core.csv (3 rows)\n",
      "[saved] figure -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\figs\\eda_transition_matrix_core.png\n",
      "[toc] Transition matrix ready: 189.15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "189.14519143104553"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8) Sequence patterns — event 3-grams and Markov transitions\n",
    "\n",
    "# Prepare\n",
    "seq_df = maybe_sample_sessions(events).sort_values([\"session_id\",\"timestamp\"])\n",
    "\n",
    "# 3-grams\n",
    "tic(\"3-gram extraction\")\n",
    "def event_ngrams(seq, n=3):\n",
    "    if len(seq) < n:\n",
    "        return []\n",
    "    return [tuple(seq[i:i+n]) for i in range(len(seq)-n+1)]\n",
    "\n",
    "ngrams_counter = Counter()\n",
    "for _, g in seq_df.groupby(\"session_id\", sort=False):\n",
    "    grams = event_ngrams(g[\"event\"].tolist(), n=3)\n",
    "    ngrams_counter.update(grams)\n",
    "\n",
    "grams_df = pd.DataFrame(ngrams_counter.items(), columns=[\"ngram\",\"count\"]).sort_values(\"count\", ascending=False)\n",
    "grams_df[\"ngram_str\"] = grams_df[\"ngram\"].apply(lambda t: \"->\".join(t))\n",
    "top_grams = grams_df.head(TOP_N).copy()\n",
    "save_table(top_grams[[\"ngram_str\",\"count\"]], TAB_DIR / \"top_event_3grams.csv\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.barh(top_grams[\"ngram_str\"][::-1], top_grams[\"count\"][::-1])\n",
    "ax.set_xlabel(\"Count\"); ax.set_ylabel(\"3-gram\"); ax.set_title(\"Top Event 3-grams\")\n",
    "save_fig(fig, \"eda_top_3grams\")\n",
    "toc(\"3-grams ready\")\n",
    "\n",
    "# First-order Markov transition matrix among core events\n",
    "tic(\"Transition matrix\")\n",
    "core = EVENT_TYPES  # ignore \"other\" for clarity\n",
    "idx = {e:i for i,e in enumerate(core)}\n",
    "m = np.zeros((len(core), len(core)), dtype=np.float64)\n",
    "\n",
    "prev = None\n",
    "prev_sess = None\n",
    "for _, row in seq_df.iterrows():\n",
    "    e = row[\"event\"]\n",
    "    s = row[\"session_id\"]\n",
    "    if e not in core:\n",
    "        prev = None; prev_sess = s\n",
    "        continue\n",
    "    if prev is not None and prev_sess == s:\n",
    "        m[idx[prev], idx[e]] += 1.0\n",
    "    prev = e; prev_sess = s\n",
    "\n",
    "# Row-normalise\n",
    "row_sums = m.sum(axis=1, keepdims=True) + 1e-12\n",
    "tm = m / row_sums\n",
    "\n",
    "tm_df = pd.DataFrame(tm, index=core, columns=core)\n",
    "save_table(tm_df.reset_index().rename(columns={\"index\":\"from\"}), TAB_DIR / \"transition_matrix_core.csv\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.5,4.5))\n",
    "im = ax.imshow(tm, aspect=\"auto\")\n",
    "ax.set_xticks(range(len(core))); ax.set_xticklabels(core, rotation=45, ha=\"right\")\n",
    "ax.set_yticks(range(len(core))); ax.set_yticklabels(core)\n",
    "ax.set_title(\"Markov Transition Probabilities (core events)\")\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "save_fig(fig, \"eda_transition_matrix_core\")\n",
    "toc(\"Transition matrix ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df277740-15af-4211-b81c-4b6b3dd45393",
   "metadata": {},
   "source": [
    "## 9) Outlier/bot heuristics and data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b23df93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tic] Outlier heuristics & data quality\n",
      "[saved] table -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\tables\\potential_automation_or_outliers.csv (1,389,829 rows)\n",
      "[saved] table -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\EDA_outputs\\tables\\data_quality_checks.csv (2 rows)\n",
      "[toc] Outlier & quality checks ready: 8.81s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.806152582168579"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9) Outlier/bot heuristics and data quality checks\n",
    "\n",
    "tic(\"Outlier heuristics & data quality\")\n",
    "\n",
    "# Outliers by events per minute and long duration\n",
    "q = sess_df[\"duration_sec\"].quantile(OUTLIER_DUR_PCTL/100.0)\n",
    "outliers = sess_df[(sess_df[\"events_per_min\"] > OUTLIER_EPM_THRESH) | (sess_df[\"duration_sec\"] > q)].copy()\n",
    "save_table(outliers, TAB_DIR / \"potential_automation_or_outliers.csv\")\n",
    "\n",
    "# Duplicates & unknowns (data qulity checks)\n",
    "dupe_rows = events.duplicated(subset=[\"session_id\",\"timestamp\",\"event\",\"itemid\"]).sum()\n",
    "unknown_events = set(events.loc[~events[\"event\"].isin(EVENT_TYPES), \"event\"].unique()) - {\"other\"}\n",
    "dq = pd.DataFrame([\n",
    "    {\"check\":\"duplicate_rows\", \"value\": int(dupe_rows)},\n",
    "    {\"check\":\"unknown_event_labels\", \"value\": \",\".join(sorted(map(str, unknown_events))) if unknown_events else \"\"}\n",
    "])\n",
    "save_table(dq, TAB_DIR / \"data_quality_checks.csv\")\n",
    "\n",
    "toc(\"Outlier & quality checks ready\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
