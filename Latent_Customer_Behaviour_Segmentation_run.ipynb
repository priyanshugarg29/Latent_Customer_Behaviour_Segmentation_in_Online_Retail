{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55480cb",
   "metadata": {},
   "source": [
    "# Latent Customer Behaviour Segmentation Implementation Pipeline\n",
    "\n",
    "**Pipeline Configuration:**\n",
    "**data -> embedding features -> clustering -> evaluation -> explainability -> business‑friendly profiles**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d29976d",
   "metadata": {},
   "source": [
    "## 0. Dependency Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e75299-e406-479d-a47f-2748fb008e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Dependencies installed successfully\n",
      "time: 0 ns (started: 2025-08-27 14:40:50 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Installing necessary libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Core scientific + ML stack libraries\n",
    "%pip install -q scipy pandas scikit-learn gensim umap-learn hdbscan shap xgboost seaborn tqdm pyarrow polars matplotlib numpy\n",
    "\n",
    "# Extra utilities being used\n",
    "%pip install -q tqdm-joblib\n",
    "%pip install -q --upgrade kaggle\n",
    "%pip install -q ipython-autotime\n",
    "\n",
    "# Load autotime extension to measure execution times automatically\n",
    "%load_ext autotime\n",
    "\n",
    "print(\"Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b88b8",
   "metadata": {},
   "source": [
    "## 1. Runtime Configuration and Data: Loading, Basic Checks, and Sessionisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2687c31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIGURATION SUMMARY\n",
      "PROJECT_NAME: Latent_Customer_Behaviour_Segmentation_Implementation_Pipeline\n",
      "RUN_ID:       20250827_144050\n",
      "Random seed:  42\n",
      "Events CSV:   C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\data\\events.csv\n",
      "PCA dims:     96 | PCA for model: True\n",
      "UMAP for viz: True (n=2, metric=cosine)\n",
      "HDBSCAN: min_cluster_size=900, min_samples=20, metric=euclidean, selection=eom, epsilon=0.0, sample=90000\n",
      "OPTICS: use=False, xi=0.05, min_samples=20, max_eps=inf\n",
      "DBSCAN: use=True, eps=None, min_samples=8, metric=euclidean\n",
      "Eval sample:  70000, DBCV=True, silhouette=True\n",
      "SHAP: use=True, sample=30000, topN=15\n",
      "Outputs -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\n",
      "Loading events and building sessions\n",
      "Rows after load/sessionise: 1,377,221\n",
      "Sessions (pre-filter):      382,780\n",
      "Visitors (pre-filter):      315,642\n",
      "[users] Dropped 314 heavy users (>99.9th pct). Users: 315,642 -> 315,328\n",
      "Outlier thresholds (p99 with floors):\n",
      "  events <= 100\n",
      "  events_per_min <= 30.00\n",
      "  duration_sec <= 10,800  (~3.0h)\n",
      "  item_diversity <= 200\n",
      "[sessions] Outlier removal: 371,121 -> 371,095 sessions (26 dropped); rows 1,235,554 -> 1,234,115\n",
      "Session features ready and aligned with filtered data.\n",
      "time: 1min 19s (started: 2025-08-27 14:40:50 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# 0) CONFIGURATIONS\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "# Project meta\n",
    "PROJECT_NAME = \"Latent_Customer_Behaviour_Segmentation_Implementation_Pipeline\"\n",
    "RUN_ID       = dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # used in file names\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Threads / parallelisation\n",
    "N_JOBS   = -1   # -1 = use all cores-1; set to fixed int for reproducibility\n",
    "\n",
    "#  Paths\n",
    "ROOT_DIR   = Path(\"C:/Users/Admin/Documents/WBS/Dissertation/Submission Related files/Notebooks/Modelling\")\n",
    "DATA_DIR   = ROOT_DIR / \"data\"\n",
    "OUT_DIR    = ROOT_DIR / \"outputs\"\n",
    "FIGS_DIR   = OUT_DIR / \"figs\"\n",
    "TABLES_DIR = OUT_DIR / \"tables\"\n",
    "SHAP_DIR   = OUT_DIR / \"shap_artifacts\"\n",
    "\n",
    "# Input files\n",
    "EVENTS_CSV = DATA_DIR / \"events.csv\"               # clickstream (session_id, user_id, item_id, timestamp, event, etc.)\n",
    "ITEMS_CSV  = DATA_DIR / \"item_properties_part1.csv\"   # optional (item metadata)\n",
    "ITEMS2_CSV = DATA_DIR / \"item_properties_part2.csv\"        # optional 2nd metadata file\n",
    "\n",
    "# Ensure output folders exist\n",
    "for p in [OUT_DIR, FIGS_DIR, TABLES_DIR, SHAP_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data schema & timestamp handling\n",
    "# Only change these if your raw file uses different column names\n",
    "COLS = {\n",
    "    \"session\":   \"session_id\",\n",
    "    \"user\":      \"user_id\",\n",
    "    \"item\":      \"item_id\",\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"event\":     \"event\"          # expected: \"view\", \"addtocart\", \"transaction\" etc.\n",
    "}\n",
    "# timestamps sometimes come in ms; if so, set TS_DIV=1000 to convert to seconds\n",
    "TS_DIV = 1000  # 1 if already epoch seconds; 1000 if epoch milliseconds\n",
    "\n",
    "# Session construction\n",
    "SESSION_GAP_SEC       = 30 * 60   # break session if gap > 30 minutes\n",
    "MIN_EVENTS_PER_SESSION= 2         # drop sessions with fewer events than this\n",
    "MAX_EVENTS_PER_SESSION= 5000      # guardrail for bots/anomalies\n",
    "DROP_ABNORMAL_USERS   = True      # remove outliers by volume (top x%)\n",
    "ABNORMAL_USER_PCTL    = 99.9      # drop users above this percentile of event count\n",
    "\n",
    "#  Representation / Embeddings\n",
    "# We combine session-level Doc2Vec + item-level Word2Vec (mean-pooled)\n",
    "USE_DOC2VEC           = True\n",
    "USE_WORD2VEC          = True\n",
    "CONCAT_DOC_W2V        = True\n",
    "\n",
    "# Extra options\n",
    "USE_SIF_WEIGHTING   = True   # Smooth Inverse Frequency weighting for W2V pooling\n",
    "USE_TFIDF_WEIGHTING = False  # TF-IDF pooling instead of SIF (rarely needed)\n",
    "USE_DBOW_EXTRA      = True   # Add small DBOW Doc2Vec for robustness\n",
    "\n",
    "# Word2Vec (items -> vector)\n",
    "W2V_VECTOR_SIZE       = 64\n",
    "W2V_WINDOW            = 5\n",
    "W2V_MIN_COUNT         = 2\n",
    "W2V_SG                = 1          # 1 = skip-gram, 0 = CBOW\n",
    "W2V_NEGATIVE          = 5\n",
    "W2V_SUBSAMPLE         = 1e-5\n",
    "W2V_EPOCHS            = 20\n",
    "\n",
    "# Doc2Vec (session sequences -> vector)\n",
    "D2V_VECTOR_SIZE       = 96\n",
    "D2V_WINDOW            = 5\n",
    "D2V_MIN_COUNT         = 2\n",
    "D2V_DM                = 1          # 1 = PV-DM, 0 = PV-DBOW\n",
    "D2V_EPOCHS            = 30\n",
    "D2V_ALPHA             = 0.025\n",
    "D2V_MIN_ALPHA         = 0.0005\n",
    "\n",
    "# Dimensionality reduction for modelling & viz\n",
    "USE_PCA_FOR_MODEL     = True\n",
    "PCA_N_COMPONENTS      = 96          # for downstream clustering (denoising)\n",
    "PCA_WHITEN            = False       # Whitening distorts the natural variance structure, reducing the ability to interpret PCA components by how much variance they explain\n",
    "PCA_RANDOM_STATE      = RANDOM_SEED\n",
    "\n",
    "USE_UMAP_FOR_VIZ      = True        # 2D viz only (do NOT use UMAP space for metrics)\n",
    "UMAP_N_NEIGHBORS      = 30\n",
    "UMAP_MIN_DIST         = 0.25\n",
    "UMAP_N_COMPONENTS     = 2\n",
    "UMAP_METRIC           = \"cosine\"\n",
    "UMAP_RANDOM_STATE     = RANDOM_SEED\n",
    "\n",
    "# Clustering (we compare multiple models)\n",
    "\n",
    "# Clustering orchestration (sampling + eval)\n",
    "USE_SAMPLED_CLUSTERING = True     # if False, fit all models on full data\n",
    "EVAL_ON_SAMPLE         = True     # compute internal metrics on a shared sample\n",
    "DO_INTERNAL_SCORES     = True     # print/save silhouette/CH/DB for quick comparisons\n",
    "\n",
    "# MiniBatch K-Means (baseline centroid method)\n",
    "KMEANS_K_GRID         = [2, 3, 4, 6, 8, 10, 12, 14]\n",
    "KMEANS_BATCH_SIZE     = 4096\n",
    "KMEANS_MAX_ITER       = 300\n",
    "KMEANS_N_INIT          = 50       # number of centroid initialisations\n",
    "\n",
    "# Gaussian Mixture Models (select K by BIC)\n",
    "GMM_K_GRID            = [8, 10, 12, 14, 16, 188]\n",
    "GMM_COV_TYPE          = \"diag\"\n",
    "GMM_MAX_ITER          = 300\n",
    "GMM_N_INIT            = 3\n",
    "GMM_RANDOM_STATE      = RANDOM_SEED\n",
    "\n",
    "# HDBSCAN (density-based; primary for segmentation)\n",
    "# We run on a stratified sample for speed, then propagate labels to full set\n",
    "HDBSCAN_SAMPLE_SIZE         = 90_000      # sample size for fit\n",
    "HDBSCAN_MIN_CLUSTER_SIZE    = 1500         # smallest segment we care about (tune 300–1000)\n",
    "HDBSCAN_MIN_SAMPLES         = 20          # higher -> stricter core definition (more noise)\n",
    "HDBSCAN_METRIC              = \"euclidean\"    # try: \"cosine\" or \"manhattan\" (often better than euclidean for embeddings)\n",
    "HDBSCAN_CLUSTER_SELECTION   = \"eom\"       # \"eom\" (stable, fewer clusters) or \"leaf\" (more fine-grained)\n",
    "HDBSCAN_SELECTION_EPSILON   = 0.0         # >0 merges very close clusters; keep 0 unless over-fragmenting\n",
    "HDBSCAN_ALLOW_SINGLE_CLUSTER= False\n",
    "HDBSCAN_CLUSTER_PROB_THRESH = 0.0         # keep all; raise to 0.15–0.3 to drop low-confidence members\n",
    "HDBSCAN_APPROX_PREDICT      = True        # assign remaining points to nearest clusters\n",
    "\n",
    "# Agglomerative + kNN propagation\n",
    "USE_AGGLO              = True\n",
    "AGGLOMERATIVE_NCL      = 12       # number of clusters for Ward linkage (tune as needed)\n",
    "K_CONNECTIVITY         = 30       # k for building connectivity graph (Ward requires euclidean)\n",
    "K_PROPAGATE            = 30       # k for kNN label propagation to full set\n",
    "\n",
    "# DBSCAN (classic)\n",
    "USE_DBSCAN                  = True\n",
    "DBSCAN_EPS                  = None        # None to evaluate from k-distance elbow (or evaluated from previous calibrations)\n",
    "DBSCAN_MIN_SAMPLES          = 8\n",
    "DBSCAN_METRIC               = \"euclidean\"\n",
    "# DBSCAN sampling for eps estimation and fit\n",
    "DBSCAN_EPS_SAMPLE_N    = 25_000  # sample size for k-distance eps estimation\n",
    "DBSCAN_FIT_SAMPLE_N    = 30_000  # sample size for DBSCAN fit when sampling + propagation\n",
    "\n",
    "# OPTICS (alternative density method)\n",
    "USE_OPTICS                  = False\n",
    "OPTICS_MIN_SAMPLES          = 20\n",
    "OPTICS_XI                   = 0.05         # smaller -> more clusters; larger -> fewer clusters\n",
    "OPTICS_MIN_CLUSTER_SIZE     = 50\n",
    "OPTICS_MAX_EPS              = np.inf       # set to a reasonable cap to speed up (e.g., 2.0) if known\n",
    "OPTICS_METRIC               = \"cosine\"\n",
    "\n",
    "# Evaluation (internal metrics + stability)\n",
    "EVAL_SAMPLE_SIZE            = 70_000       # for fast metric computation on large sets\n",
    "COMPUTE_SILHOUETTE          = True         # only on non-noise labels\n",
    "COMPUTE_CH_DB               = True         # Calinski–Harabasz & Davies–Bouldin\n",
    "COMPUTE_DBCV                = False         # density-based validity (better for HDBSCAN)\n",
    "NOISE_LABEL                 = -1\n",
    "\n",
    "# Temporal stability (early vs late split)\n",
    "CHECK_TEMPORAL_DRIFT        = True\n",
    "EARLY_LATE_SPLIT_PCT        = 0.5          # 0.5 = first half vs second half by time\n",
    "\n",
    "# Cross-model agreement (optional)\n",
    "COMPUTE_ARI_AMI             = True\n",
    "\n",
    "# Output paths for clustering evaluations\n",
    "def run_stem(name: str) -> Path:\n",
    "    return OUT_DIR / f\"{RUN_ID}_{name}\"\n",
    "SCORECARD_OUT          = run_stem(\"cluster_scorecard.csv\")\n",
    "\n",
    "# Profiling & naming clusters\n",
    "# These are simple session-level features for interpretation/profiling tables\n",
    "PROFILE_FEATURES = [\n",
    "    \"events\", \"view\", \"addtocart\", \"transaction\",\n",
    "    \"duration_sec\", \"events_per_min\", \"item_diversity\",\n",
    "    \"last_evt_view\", \"last_evt_addtocart\", \"last_evt_transaction\",\n",
    "    \"hour\", \"dow\", \"is_evening\"\n",
    "]\n",
    "\n",
    "# RFM-style baseline (for a simple, transparent benchmark)\n",
    "BASELINE_LEVEL              = \"session\"    # or \"visitor\"\n",
    "MONETARY_MODE               = \"transactions\"  # \"transactions\" or \"none\"\n",
    "K_GRID_BASE                 = [4, 6, 8, 10, 12]\n",
    "SIL_SAMPLE_N = 5_000\n",
    "ATC_WEIGHT   = 0.5   # add-to-cart counts as “half” a purchase in M proxy\n",
    "\n",
    "#  SHAP (surrogate explainability for cluster membership)\n",
    "USE_SHAP_SURROGATE          = True\n",
    "SHAP_SAMPLE_SIZE            = 30_000      # sample for surrogate modelling\n",
    "SHAP_TOP_N_FEATURES         = 15           # report top-N features globally + per-cluster\n",
    "RF_N_ESTIMATORS             = 500\n",
    "RF_MAX_DEPTH                = 12\n",
    "RF_RANDOM_STATE             = RANDOM_SEED\n",
    "SHAP_BACKGROUND_SIZE        = 2_000        # interventional background size for TreeSHAP\n",
    "SAVE_BEESWARM_PNG           = True\n",
    "\n",
    "# Saving and file naming\n",
    "SAVE_CSV_SUMMARIES          = True\n",
    "SAVE_VIZ_PNG                = True\n",
    "FIG_DPI                     = 150\n",
    "\n",
    "#  Utility: consistent file stem for this run\n",
    "def run_stem(name: str) -> Path:\n",
    "    \"\"\"Create a consistent file stem under outputs/ with run id and name.\"\"\"\n",
    "    return OUT_DIR / f\"{RUN_ID}_{name}\"\n",
    "\n",
    "# printing configuration summary for current run\n",
    "def print_config_summary():\n",
    "    print(\"CONFIGURATION SUMMARY\")\n",
    "    print(f\"PROJECT_NAME: {PROJECT_NAME}\")\n",
    "    print(f\"RUN_ID:       {RUN_ID}\")\n",
    "    print(f\"Random seed:  {RANDOM_SEED}\")\n",
    "    print(f\"Events CSV:   {EVENTS_CSV}\")\n",
    "    print(f\"PCA dims:     {PCA_N_COMPONENTS} | PCA for model: {USE_PCA_FOR_MODEL}\")\n",
    "    print(f\"UMAP for viz: {USE_UMAP_FOR_VIZ} (n={UMAP_N_COMPONENTS}, metric={UMAP_METRIC})\")\n",
    "    print(f\"HDBSCAN: min_cluster_size={HDBSCAN_MIN_CLUSTER_SIZE}, min_samples={HDBSCAN_MIN_SAMPLES}, \"\n",
    "          f\"metric={HDBSCAN_METRIC}, selection={HDBSCAN_CLUSTER_SELECTION}, \"\n",
    "          f\"epsilon={HDBSCAN_SELECTION_EPSILON}, sample={HDBSCAN_SAMPLE_SIZE}\")\n",
    "    print(f\"OPTICS: use={USE_OPTICS}, xi={OPTICS_XI}, min_samples={OPTICS_MIN_SAMPLES}, max_eps={OPTICS_MAX_EPS}\")\n",
    "    print(f\"DBSCAN: use={USE_DBSCAN}, eps={DBSCAN_EPS}, min_samples={DBSCAN_MIN_SAMPLES}, metric={DBSCAN_METRIC}\")\n",
    "    print(f\"Eval sample:  {EVAL_SAMPLE_SIZE}, DBCV={COMPUTE_DBCV}, silhouette={COMPUTE_SILHOUETTE}\")\n",
    "    print(f\"SHAP: use={USE_SHAP_SURROGATE}, sample={SHAP_SAMPLE_SIZE}, topN={SHAP_TOP_N_FEATURES}\")\n",
    "    print(f\"Outputs -> {OUT_DIR}\")\n",
    "\n",
    "\n",
    "print_config_summary()\n",
    "\n",
    "\n",
    "# Loading data \n",
    "\n",
    "print(\"Loading events and building sessions\")\n",
    "\n",
    "# 1) Load events (minimal schema)\n",
    "\n",
    "usecols = [\"visitorid\", \"timestamp\", \"event\", \"itemid\"]\n",
    "events = pd.read_csv(EVENTS_CSV, usecols=usecols, nrows=None)\n",
    "\n",
    "# Basic cleaning:\n",
    "# - drop rows missing core fields\n",
    "# - keep timestamps as int64 (UNIX seconds)\n",
    "events = events.dropna(subset=[\"visitorid\", \"timestamp\", \"event\"]).copy()\n",
    "\n",
    "# If timestamps are in milliseconds, downscale to seconds (TS_DIV from CONFIG)\n",
    "ts = events[\"timestamp\"].astype(\"int64\")\n",
    "if ts.max() > 10**11 or TS_DIV == 1000:  # rough check + config guardrail\n",
    "    events[\"timestamp\"] = (ts // 1000).astype(\"int64\")\n",
    "else:\n",
    "    events[\"timestamp\"] = ts\n",
    "\n",
    "# Normalise event labels (keeps downstream logic simple)\n",
    "events[\"event\"] = (\n",
    "    events[\"event\"]\n",
    "    .astype(\"string\")\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .replace({\n",
    "        \"add-to-cart\": \"addtocart\",\n",
    "        \"add_to_cart\": \"addtocart\",\n",
    "        \"cart\": \"addtocart\",\n",
    "        \"purchase\": \"transaction\",\n",
    "        \"buy\": \"transaction\",\n",
    "    })\n",
    ")\n",
    "\n",
    "# Sorting is important for session building\n",
    "events = events.sort_values([\"visitorid\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "# 2) Build sessions (30 min inactivity gap)\n",
    "\n",
    "gap = SESSION_GAP_SEC \n",
    "# New session when gap between consecutive events for the same visitor > threshold\n",
    "dt = events.groupby(\"visitorid\")[\"timestamp\"].diff().fillna(gap + 1)\n",
    "is_new_session = (dt > gap) | events[\"visitorid\"].ne(events[\"visitorid\"].shift())\n",
    "events[\"session_id\"] = np.cumsum(is_new_session)\n",
    "\n",
    "# Dropping super-short sessions (e.g., single-click noise)\n",
    "if MIN_EVENTS_PER_SESSION and MIN_EVENTS_PER_SESSION > 1:\n",
    "    sizes = events.groupby(\"session_id\").size()\n",
    "    keep = sizes[sizes >= MIN_EVENTS_PER_SESSION].index\n",
    "    events = events[events[\"session_id\"].isin(keep)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Rows after load/sessionise: {len(events):,}\")\n",
    "print(f\"Sessions (pre-filter):      {events['session_id'].nunique():,}\")\n",
    "print(f\"Visitors (pre-filter):      {events['visitorid'].nunique():,}\")\n",
    "\n",
    "\n",
    "# 3) Helper function to compute per-session features for profiling\n",
    "\n",
    "def compute_session_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns one row per session_id with:\n",
    "    - events, duration_sec, events_per_min\n",
    "    - item_diversity (unique items)\n",
    "    - last event flags (view/addtocart/transaction)\n",
    "    - basic time features (hour, dow, is_evening)\n",
    "    \"\"\"\n",
    "    agg = {\n",
    "        \"timestamp\": [\"min\", \"max\", \"count\"],\n",
    "        \"itemid\": pd.Series.nunique,\n",
    "    }\n",
    "    g = df.groupby(\"session_id\").agg(agg)\n",
    "    g.columns = [\"ts_min\", \"ts_max\", \"events\", \"item_diversity\"]\n",
    "    g[\"duration_sec\"] = (g[\"ts_max\"] - g[\"ts_min\"]).clip(lower=0)\n",
    "    g[\"events_per_min\"] = g[\"events\"] / np.maximum(1, g[\"duration_sec\"] / 60.0)\n",
    "\n",
    "    # Last event per session\n",
    "    last_evt = (\n",
    "        df.sort_values([\"session_id\", \"timestamp\"])\n",
    "          .groupby(\"session_id\")[\"event\"].last()\n",
    "    )\n",
    "    for ev in [\"view\", \"addtocart\", \"transaction\"]:\n",
    "        g[f\"last_evt_{ev}\"] = (last_evt == ev).astype(int)\n",
    "\n",
    "    # Lightweight time-of-day features based on last timestamp\n",
    "    last_ts = (\n",
    "        df.sort_values([\"session_id\", \"timestamp\"])\n",
    "          .groupby(\"session_id\")[\"timestamp\"].last()\n",
    "    )\n",
    "    last_dt = pd.to_datetime(last_ts, unit=\"s\", utc=True).dt.tz_convert(None)\n",
    "    g[\"hour\"] = last_dt.dt.hour\n",
    "    g[\"dow\"] = last_dt.dt.dayofweek\n",
    "    g[\"is_evening\"] = g[\"hour\"].between(17, 22).astype(int)\n",
    "\n",
    "    return g.reset_index()\n",
    "\n",
    "session_feats = compute_session_features(events)\n",
    "\n",
    "\n",
    "# 4) dropping abnormal users by volume\n",
    "DROP_ABNORMAL_USERS = False\n",
    "if DROP_ABNORMAL_USERS and \"visitorid\" in events.columns:\n",
    "    before_users = events[\"visitorid\"].nunique()\n",
    "    user_events = events.groupby(\"visitorid\").size()\n",
    "    cutoff = np.percentile(user_events, ABNORMAL_USER_PCTL)\n",
    "    heavy_users = user_events[user_events > cutoff].index\n",
    "\n",
    "    if len(heavy_users) > 0:\n",
    "        events = events.loc[~events[\"visitorid\"].isin(heavy_users)].reset_index(drop=True)\n",
    "        print(\n",
    "            f\"[users] Dropped {len(heavy_users):,} heavy users \"\n",
    "            f\"(>{ABNORMAL_USER_PCTL}th pct). Users: {before_users:,} -> {events['visitorid'].nunique():,}\"\n",
    "        )\n",
    "        # Recomputing session features (as some sessions might have vanished)\n",
    "        session_feats = compute_session_features(events)\n",
    "\n",
    "\n",
    "# 5) Defining outlier thresholds from data (p99 and safe floors threshold values)\n",
    "\n",
    "def p99(series: pd.Series) -> float:\n",
    "    return float(np.nanpercentile(series.values, 99))\n",
    "\n",
    "p99_events       = p99(session_feats[\"events\"])\n",
    "p99_epm          = p99(session_feats[\"events_per_min\"])\n",
    "p99_duration_sec = p99(session_feats[\"duration_sec\"])\n",
    "p99_itemdiv      = p99(session_feats[\"item_diversity\"])\n",
    "\n",
    "# Defining floors helps in keeping thresholds sensible\n",
    "THRESH_EVENTS_MAX   = max(100, int(p99_events))\n",
    "THRESH_EPM_MAX      = max(30.0, p99_epm)\n",
    "THRESH_DURATION_MAX = max(3 * 3600, int(p99_duration_sec))  # at least 3 hours\n",
    "THRESH_ITEMDIV_MAX  = max(200, int(p99_itemdiv))\n",
    "\n",
    "print(\"Outlier thresholds (p99 with floors):\")\n",
    "print(f\"  events <= {THRESH_EVENTS_MAX:,}\")\n",
    "print(f\"  events_per_min <= {THRESH_EPM_MAX:,.2f}\")\n",
    "print(f\"  duration_sec <= {THRESH_DURATION_MAX:,}  (~{THRESH_DURATION_MAX/3600:.1f}h)\")\n",
    "print(f\"  item_diversity <= {THRESH_ITEMDIV_MAX:,}\")\n",
    "\n",
    "\n",
    "# 6) Flagging and removing potential outliers\n",
    "\n",
    "outlier_mask = (\n",
    "    (session_feats[\"events\"] > THRESH_EVENTS_MAX) |\n",
    "    (session_feats[\"events_per_min\"] > THRESH_EPM_MAX) |\n",
    "    (session_feats[\"duration_sec\"] > THRESH_DURATION_MAX) |\n",
    "    (session_feats[\"item_diversity\"] > THRESH_ITEMDIV_MAX)\n",
    ")\n",
    "potential_outliers = session_feats.loc[outlier_mask].copy()\n",
    "\n",
    "before_rows = len(events)\n",
    "before_sessions = events[\"session_id\"].nunique()\n",
    "\n",
    "kept_session_ids = set(session_feats.loc[~outlier_mask, \"session_id\"].values)\n",
    "events = events[events[\"session_id\"].isin(kept_session_ids)].reset_index(drop=True)\n",
    "\n",
    "after_rows = len(events)\n",
    "after_sessions = events[\"session_id\"].nunique()\n",
    "\n",
    "print(\n",
    "    f\"[sessions] Outlier removal: {before_sessions:,} -> {after_sessions:,} sessions \"\n",
    "    f\"({before_sessions - after_sessions:,} dropped); rows {before_rows:,} -> {after_rows:,}\"\n",
    ")\n",
    "\n",
    "# Keeping features aligned with filtered events\n",
    "session_feats = compute_session_features(events)\n",
    "\n",
    "\n",
    "# 7) Saving for audit and dashboard\n",
    "# Percentiles\n",
    "pct = (session_feats[[\"events\", \"events_per_min\", \"duration_sec\", \"item_diversity\"]].describe(percentiles=[0.5, 0.9, 0.95, 0.99]).round(3))\n",
    "pct_path = TABLES_DIR / f\"{RUN_ID}_session_feature_percentiles.csv\"\n",
    "pct.to_csv(pct_path)\n",
    "\n",
    "# Outliers we removed (transparency)\n",
    "out_path = TABLES_DIR / f\"{RUN_ID}_potential_automation_or_outliers.csv\"\n",
    "potential_outliers.to_csv(out_path, index=False)\n",
    "\n",
    "# Dataset overview (post-cleaning)\n",
    "overview = pd.DataFrame([{\n",
    "    \"rows\": after_rows,\n",
    "    \"sessions\": after_sessions,\n",
    "    \"visitors\": events[\"visitorid\"].nunique() if \"visitorid\" in events.columns else np.nan,\n",
    "    \"items\": events[\"itemid\"].nunique(),\n",
    "    \"date_start\": pd.to_datetime(events[\"timestamp\"], unit=\"s\").min(),\n",
    "    \"date_end\": pd.to_datetime(events[\"timestamp\"], unit=\"s\").max()\n",
    "}])\n",
    "ov_path = TABLES_DIR / f\"{RUN_ID}_dataset_overview.csv\"\n",
    "overview.to_csv(ov_path, index=False)\n",
    "\n",
    "# Building a simple session-level funnel (post-cleaning)\n",
    "evt_flags = (\n",
    "    events.assign(flag=1)\n",
    "        .pivot_table(index=\"session_id\", columns=\"event\",\n",
    "        values=\"flag\", aggfunc=\"sum\", fill_value=0)\n",
    ")\n",
    "# Ensuring all columns exist\n",
    "for c in [\"view\", \"addtocart\", \"transaction\"]:\n",
    "    if c not in evt_flags.columns:\n",
    "        evt_flags[c] = 0\n",
    "# Saving funnel data\n",
    "funnel = pd.DataFrame({\n",
    "    \"stage\": [\"view\", \"addtocart\", \"transaction\"],\n",
    "    \"sessions_reached\": [\n",
    "        int((evt_flags[\"view\"] > 0).sum()),\n",
    "        int(((evt_flags[\"view\"] > 0) & (evt_flags[\"addtocart\"] > 0)).sum()),\n",
    "        int(((evt_flags[\"addtocart\"] > 0) & (evt_flags[\"transaction\"] > 0)).sum())\n",
    "    ]\n",
    "})\n",
    "funnel[\"prop_of_total\"] = funnel[\"sessions_reached\"] / after_sessions\n",
    "fun_path = TABLES_DIR / f\"{RUN_ID}_funnel_session_level.csv\"\n",
    "funnel.to_csv(fun_path, index=False)\n",
    "\n",
    "print(\"Session features ready and aligned with filtered data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170df9dc",
   "metadata": {},
   "source": [
    "## 2. Deriving features via neural embeddings (session‑level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd316aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Starting: Preparing token sequences per session\n",
      "token sequences per session prepared.\n",
      "completed: Preparing token sequences per session in 8.68s\n",
      "\n",
      ">>> Starting: Building frequency dictionaries\n",
      "completed: Building frequency dictionaries in 0.30s\n",
      "\n",
      ">>> Starting: Training Word2Vec + pooling\n",
      "Word2Vec training completed.\n",
      "completed: Training Word2Vec + pooling in 28.45s\n",
      "\n",
      ">>> Starting: Training Doc2Vec (DM) [and optional DBOW]\n",
      "Doc2Vec training completed.\n",
      "completed: Training Doc2Vec (DM) [and optional DBOW] in 1458.71s\n",
      "\n",
      ">>> Starting: Concatenating feature blocks\n",
      "Concatenation completed. Raw feature shape: (371095, 208)\n",
      "completed: Concatenating feature blocks in 0.08s\n",
      "\n",
      ">>> Starting: Standardising & L2-normalising\n",
      "Standardisation and L2-normalisation completed.\n",
      "Feature matrix (scaled) shape: (371095, 208)\n",
      "Feature matrix (L2) shape: (371095, 208)\n",
      "completed: Standardising & L2-normalising in 1.09s\n",
      "\n",
      ">>> Starting: Health checks\n",
      "W2V vocab size: 92956\n",
      "Zero-vector sessions from W2V pooling: 3646\n",
      "completed: Health checks in 0.01s\n",
      "time: 24min 58s (started: 2025-08-27 14:42:09 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# 2) Session-level neural embeddings\n",
    "\n",
    "\n",
    "# 1) Imports\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math, os, time\n",
    "\n",
    "# helpers for timing\n",
    "def tic(section):\n",
    "    print(f\"\\n>>> Starting: {section}\")\n",
    "    return time.perf_counter()\n",
    "\n",
    "def toc(t0, section):\n",
    "    dt = time.perf_counter() - t0\n",
    "    print(f\"completed: {section} in {dt:.2f}s\")\n",
    "\n",
    "# worker threads logic\n",
    "if isinstance(N_JOBS, int) and N_JOBS == -1:\n",
    "    THREADS = max(1, (os.cpu_count() or 2) - 1)\n",
    "elif isinstance(N_JOBS, int) and N_JOBS > 0:\n",
    "    THREADS = N_JOBS\n",
    "else:\n",
    "    THREADS = 1\n",
    "\n",
    "# 2) Token sequences per session (items as strings)\n",
    "t0 = tic(\"Preparing token sequences per session\")\n",
    "events_seq = events.sort_values([\"session_id\", \"timestamp\"], kind=\"mergesort\")\n",
    "session_to_items = (\n",
    "    events_seq.groupby(\"session_id\", sort=False)[\"itemid\"]\n",
    "              .apply(lambda s: [str(x) for x in s.values])\n",
    ")\n",
    "sentences = session_to_items.tolist()\n",
    "print(\"token sequences per session prepared.\")\n",
    "toc(t0, \"Preparing token sequences per session\")\n",
    "\n",
    "# 3) Frequency dictionaries (for SIF/TF-IDF pooling)\n",
    "t0 = tic(\"Building frequency dictionaries\")\n",
    "global_counts = Counter(t for seq in sentences for t in seq)\n",
    "N_tokens      = sum(global_counts.values())\n",
    "N_docs        = len(sentences)\n",
    "df_counts     = Counter(t for seq in map(set, sentences) for t in seq) if USE_TFIDF_WEIGHTING else None\n",
    "toc(t0, \"Building frequency dictionaries\")\n",
    "\n",
    "# SIF weight function\n",
    "a_sif = 5e-4\n",
    "def sif_weight(token: str) -> float:\n",
    "    pw = global_counts[token] / max(1, N_tokens)\n",
    "    return a_sif / (a_sif + pw)\n",
    "\n",
    "# TF-IDF helper\n",
    "def tfidf_pool(tokens, get_vec):\n",
    "    if not tokens:\n",
    "        return None, 0.0\n",
    "    cnt = Counter(tokens)\n",
    "    L = float(len(tokens))\n",
    "    acc = None; wsum = 0.0\n",
    "    for t, tf in cnt.items():\n",
    "        vec = get_vec(t)\n",
    "        if vec is None:\n",
    "            continue\n",
    "        idf = math.log((N_docs + 1) / (df_counts[t] + 1)) + 1.0\n",
    "        w = (tf / L) * idf\n",
    "        acc = (vec * w) if acc is None else (acc + vec * w)\n",
    "        wsum += w\n",
    "    return acc, wsum\n",
    "\n",
    "# 4) Word2Vec + pooling\n",
    "t0 = tic(\"Training Word2Vec + pooling\")\n",
    "w2v_features = None\n",
    "if USE_WORD2VEC:\n",
    "    w2v = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=W2V_VECTOR_SIZE,\n",
    "        window=W2V_WINDOW,\n",
    "        min_count=W2V_MIN_COUNT,\n",
    "        workers=THREADS,\n",
    "        sg=W2V_SG,\n",
    "        negative=W2V_NEGATIVE,\n",
    "        sample=W2V_SUBSAMPLE,\n",
    "        epochs=W2V_EPOCHS,\n",
    "        seed=RANDOM_SEED,\n",
    "        hs=0\n",
    "    )\n",
    "\n",
    "    # pooling rules\n",
    "    def mean_pool(tokens):\n",
    "        vs = [w2v.wv[t] for t in tokens if t in w2v.wv]\n",
    "        return (np.mean(vs, axis=0) if vs else None), 1.0\n",
    "\n",
    "    def sif_pool(tokens):\n",
    "        acc = None; wsum = 0.0\n",
    "        for t in tokens:\n",
    "            if t in w2v.wv:\n",
    "                v = w2v.wv[t]\n",
    "                w = sif_weight(t)\n",
    "                acc = (v * w) if acc is None else (acc + v * w)\n",
    "                wsum += w\n",
    "        return acc, wsum\n",
    "\n",
    "    def get_vec(token):\n",
    "        return w2v.wv[token] if token in w2v.wv else None\n",
    "\n",
    "    def pooled_vector(tokens):\n",
    "        if USE_TFIDF_WEIGHTING:\n",
    "            acc, wsum = tfidf_pool(tokens, get_vec)\n",
    "        elif USE_SIF_WEIGHTING:\n",
    "            acc, wsum = sif_pool(tokens)\n",
    "        else:\n",
    "            acc, wsum = mean_pool(tokens)\n",
    "        if (acc is None) or (wsum <= 0):\n",
    "            return np.zeros(W2V_VECTOR_SIZE, dtype=\"float32\")\n",
    "        return (acc / wsum).astype(\"float32\")\n",
    "\n",
    "    w2v_features = np.vstack([pooled_vector(tokens) for tokens in sentences])\n",
    "\n",
    "print(\"Word2Vec training completed.\")\n",
    "toc(t0, \"Training Word2Vec + pooling\")\n",
    "\n",
    "# 5) Doc2Vec (DM + optional DBOW)\n",
    "t0 = tic(\"Training Doc2Vec (DM) [and optional DBOW]\")\n",
    "d2v_features_list = []\n",
    "if USE_DOC2VEC:\n",
    "    docs = [TaggedDocument(words=toks, tags=[str(sid)])\n",
    "            for sid, toks in zip(session_to_items.index.values, sentences)]\n",
    "\n",
    "    # Distributed Memory\n",
    "    d2v_dm = Doc2Vec(\n",
    "        documents=docs,\n",
    "        vector_size=D2V_VECTOR_SIZE,\n",
    "        window=D2V_WINDOW,\n",
    "        min_count=D2V_MIN_COUNT,\n",
    "        workers=THREADS,\n",
    "        dm=D2V_DM, dm_mean=1,\n",
    "        negative=W2V_NEGATIVE,\n",
    "        epochs=D2V_EPOCHS,\n",
    "        seed=RANDOM_SEED\n",
    "    )\n",
    "    d2v_dm_feats = np.vstack([d2v_dm.dv[str(sid)] for sid in session_to_items.index.values]).astype(\"float32\")\n",
    "    d2v_features_list.append(d2v_dm_feats)\n",
    "\n",
    "    # DBOW for robustness on short sessions\n",
    "    if USE_DBOW_EXTRA:\n",
    "        d2v_dbow = Doc2Vec(\n",
    "            documents=docs,\n",
    "            vector_size=max(32, D2V_VECTOR_SIZE // 2),\n",
    "            window=D2V_WINDOW,\n",
    "            min_count=D2V_MIN_COUNT,\n",
    "            workers=THREADS,\n",
    "            dm=0, dbow_words=0,\n",
    "            negative=W2V_NEGATIVE,\n",
    "            epochs=max(10, D2V_EPOCHS // 2),\n",
    "            seed=RANDOM_SEED\n",
    "        )\n",
    "        d2v_dbow_feats = np.vstack([d2v_dbow.dv[str(sid)] for sid in session_to_items.index.values]).astype(\"float32\")\n",
    "        d2v_features_list.append(d2v_dbow_feats)\n",
    "\n",
    "d2v_features = np.hstack(d2v_features_list) if d2v_features_list else None\n",
    "print(\"Doc2Vec training completed.\")\n",
    "toc(t0, \"Training Doc2Vec (DM) [and optional DBOW]\")\n",
    "\n",
    "# 6) Concatenate embeddings\n",
    "t0 = tic(\"Concatenating feature blocks\")\n",
    "feat_blocks = [f for f in (w2v_features, d2v_features) if f is not None]\n",
    "assert len(feat_blocks) >= 1, \"No embeddings produced, check USE_WORD2VEC / USE_DOC2VEC.\"\n",
    "X = feat_blocks[0] if len(feat_blocks) == 1 else np.hstack(feat_blocks)\n",
    "print(\"Concatenation completed. Raw feature shape:\", X.shape)\n",
    "toc(t0, \"Concatenating feature blocks\")\n",
    "\n",
    "# 7) Standardise + L2-normalise\n",
    "t0 = tic(\"Standardising & L2-normalising\")\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_scaled = scaler.fit_transform(X).astype(\"float32\", copy=False)\n",
    "row_norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "X_l2 = X / np.clip(row_norms, 1e-12, None)\n",
    "print(\"Standardisation and L2-normalisation completed.\")\n",
    "print(\"Feature matrix (scaled) shape:\", X_scaled.shape)\n",
    "print(\"Feature matrix (L2) shape:\", X_l2.shape)\n",
    "toc(t0, \"Standardising & L2-normalising\")\n",
    "\n",
    "# 8) Health checks\n",
    "t0 = tic(\"Health checks\")\n",
    "n_zero_w2v = 0 if w2v_features is None else int((w2v_features == 0).all(axis=1).sum())\n",
    "vocab_size = 0 if not USE_WORD2VEC else len(w2v.wv)\n",
    "print(\"W2V vocab size:\", vocab_size)\n",
    "print(\"Zero-vector sessions from W2V pooling:\", n_zero_w2v)\n",
    "toc(t0, \"Health checks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0150ddd",
   "metadata": {},
   "source": [
    "## 3. Dimensionality reduction of derived neural embeddings (PCA AND UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e25ce275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA | components: 96 | total explained variance: 0.806\n",
      "k-means/HDBSCAN input shape: (371095, 96)\n",
      "GMM input shape: (371095, 96)\n",
      "DBSCAN sample size: 90000\n",
      "HDBSCAN sample size: 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\seg\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP -> (371095, 2)\n",
      "Note: UMAP is for visual inspection only; global distances/densities are not faithful.\n",
      "UMAP plot saved to: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_sessions_2d.png\n",
      "time: 9min 24s (started: 2025-08-27 15:07:07 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# 3) Dimensionality reduction (PCA + UMAP)\n",
    "\n",
    "# 1) Imports\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 2) PCA on scaled features (denoise + numerical stability)\n",
    "X_model = X_scaled\n",
    "pca_dim_eff = int(min(PCA_N_COMPONENTS, X_scaled.shape[1]))\n",
    "assert pca_dim_eff >= 2, \"PCA_N_COMPONENTS must be >= 2.\"\n",
    "\n",
    "pca = PCA(\n",
    "    n_components=pca_dim_eff,\n",
    "    whiten=PCA_WHITEN,\n",
    "    random_state=PCA_RANDOM_STATE\n",
    ")\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Cosine-friendly copy (L2 after PCA): Euclidean is approximately same as cosine\n",
    "row_norms_pca = np.linalg.norm(X_pca, axis=1, keepdims=True)\n",
    "X_pca_l2 = X_pca / np.clip(row_norms_pca, 1e-12, None)\n",
    "\n",
    "# Downstream model inputs (choose L2 if metric is cosine)\n",
    "X_model_kmeans  = X_pca_l2                            # k-means prefers L2\n",
    "X_model_gmm     = X_pca                               # GMM prefers PCA/standardised space\n",
    "X_model_hdbscan = X_pca_l2 if HDBSCAN_METRIC == \"cosine\" else X_pca\n",
    "X_model_dbscan  = X_pca_l2 if DBSCAN_METRIC  == \"cosine\" else X_pca\n",
    "\n",
    "# Sample sizes for any later calibration/fit subsampling\n",
    "DBSCAN_SAMPLE_N  = min(90_000, X_model_dbscan.shape[0])\n",
    "HDBSCAN_SAMPLE_N = min(HDBSCAN_SAMPLE_SIZE, X_model_hdbscan.shape[0])  # tied to configuration\n",
    "\n",
    "# Reporting\n",
    "expl_var = float(np.sum(pca.explained_variance_ratio_))\n",
    "print(f\"PCA | components: {pca_dim_eff} | total explained variance: {expl_var:.3f}\")\n",
    "print(\"k-means/HDBSCAN input shape:\", X_model_kmeans.shape)\n",
    "print(\"GMM input shape:\", X_model_gmm.shape)\n",
    "print(\"DBSCAN sample size:\", DBSCAN_SAMPLE_N)\n",
    "print(\"HDBSCAN sample size:\", HDBSCAN_SAMPLE_N)\n",
    "\n",
    "# 3) UMAP for 2D visualisation (for plots only; not used for modelling)\n",
    "umap_2d = None\n",
    "if USE_UMAP_FOR_VIZ:\n",
    "    umap_2d = umap.UMAP(\n",
    "        n_neighbors=UMAP_N_NEIGHBORS,\n",
    "        min_dist=UMAP_MIN_DIST,\n",
    "        n_components=2,\n",
    "        metric=UMAP_METRIC,            # from config (e.g., \"cosine\")\n",
    "        random_state=UMAP_RANDOM_STATE\n",
    "    ).fit_transform(X_pca)             # run on denoised PCA space\n",
    "\n",
    "    print(\"UMAP ->\", umap_2d.shape)\n",
    "    print(\"Ethical Note: UMAP is for visual inspection only; global distances/densities are not faithful.\")\n",
    "\n",
    "# 4) Save UMAP scatter to FIGS_DIR (with optional downsampling)\n",
    "if USE_UMAP_FOR_VIZ and umap_2d is not None:\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    UMAP_PLOT_MAX = 250_000\n",
    "    n_points = umap_2d.shape[0]\n",
    "    if n_points > UMAP_PLOT_MAX:\n",
    "        idx = rng.choice(n_points, size=UMAP_PLOT_MAX, replace=False)\n",
    "        umap_plot = umap_2d[idx]\n",
    "    else:\n",
    "        umap_plot = umap_2d\n",
    "\n",
    "    FIGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    plt.figure(figsize=(8, 8), dpi=200)\n",
    "    plt.scatter(umap_plot[:, 0], umap_plot[:, 1], s=1, alpha=0.5, linewidths=0, rasterized=True)\n",
    "    plt.title(\"UMAP of Sessions (2D projection)\")\n",
    "    plt.xlabel(\"UMAP-1\"); plt.ylabel(\"UMAP-2\")\n",
    "    plt.tight_layout()\n",
    "    out_path = FIGS_DIR / f\"{RUN_ID}_umap_sessions_2d.png\"\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(\"UMAP plot saved to:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de008153",
   "metadata": {},
   "source": [
    "## 4. Clustering (multiple unsupervised clustering models compared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7353ae-7162-45f3-98f1-3fcbbbef8d5e",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Starting: MiniBatch K-Means\n",
      "K-Means tried: [2, 3, 4, 6, 8, 10, 12, 14] | best k: 2 | silhouette(eval): 0.097\n",
      "completed: MiniBatch K-Means in 657.88s\n",
      "\n",
      ">>> Starting: Gaussian Mixture Models (BIC)\n",
      "GMM best k by BIC: 16 | BIC: 101105230.08\n",
      "completed: Gaussian Mixture Models (BIC) in 212.63s\n",
      "\n",
      ">>> Starting: HDBSCAN + propagation\n",
      "[HDBSCAN] fit_size=90,000/371,095 | min_cluster_size=900 | min_samples=20 | metric=euclidean | selection=eom\n",
      "HDBSCAN clusters (excl. noise): 2\n",
      "HDBSCAN eval: {'sil': 0.1318977028131485, 'ch': 755.7602246410472, 'db': 1.726007524586368}\n",
      "completed: HDBSCAN + propagation in 9160.75s\n",
      "\n",
      ">>> Starting: Agglomerative (Ward) + kNN propagation\n",
      "[Agglo] building connectivity graph (n=60,000, k=30)\n",
      "[Agglo] fitting Ward linkage with n_clusters=12\n",
      "Agglomerative: propagating labels to full dataset via kNN\n",
      "Agglomerative clusters: 12\n",
      "Agglomerative eval: {'sil': 0.06385967135429382, 'ch': 1529.730591965463, 'db': 3.6492577022254653}\n",
      "completed: Agglomerative (Ward) + kNN propagation in 205.89s\n",
      "\n",
      ">>> Starting: DBSCAN (eps via k-distance) + optional propagation\n",
      "[DBSCAN] estimating eps via k-distance on 25,000 points (k=8, metric=euclidean)\n",
      "DBSCAN chosen eps: 14.11645 | silhouette(eval): None\n",
      "DBSCAN: propagating labels to full dataset via kNN...\n",
      "DBSCAN clusters (excl. noise): 1\n",
      "DBSCAN eval: {'sil': nan, 'ch': nan, 'db': nan}\n",
      "completed: DBSCAN (eps via k-distance) + optional propagation in 386.81s\n",
      "\n",
      ">>> Starting: Score summary + save\n",
      "Saved scorecard -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_cluster_scorecard.csv\n",
      "                    model  silhouette           ch        db  n_clusters  \\\n",
      "0                kmeans_2    0.097009  8064.424530  2.624744           2   \n",
      "1                  gmm_16    0.008835   852.811313  6.736936          16   \n",
      "2                 hdbscan    0.131898   755.760225  1.726008           2   \n",
      "3             agg_ward_12    0.063860  1529.730592  3.649258          12   \n",
      "4  dbscan_eps14.11645_ms8         NaN          NaN       NaN           1   \n",
      "\n",
      "   noise_pct  \n",
      "0   0.000000  \n",
      "1   0.000000  \n",
      "2   0.546216  \n",
      "3   0.000000  \n",
      "4   0.002846  \n",
      "completed: Score summary + save in 299.57s\n",
      "\n",
      ">>> Starting: Saving labels (full)\n",
      "Saved labels -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_cluster_labels_all_models.csv\n",
      "Clusterers saved: ['kmeans_2', 'kmeans_3', 'kmeans_4', 'kmeans_6', 'kmeans_8', 'kmeans_10', 'kmeans_12', 'kmeans_14', 'gmm_3', 'gmm_4', 'gmm_6', 'gmm_8', 'gmm_10', 'gmm_12', 'gmm_14', 'gmm_16', 'hdbscan', 'hdbscan_conf', 'agg_ward_12', 'dbscan_eps14.11645_ms8']\n",
      "completed: Saving labels (full) in 1.95s\n",
      "time: 3h 2min 5s (started: 2025-08-27 15:16:32 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# 4) Clustering (KMeans, GMM[BIC], HDBSCAN + propagation, Agglomerative + propagation, DBSCAN + eps tuning)\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering, DBSCAN as SK_DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier, kneighbors_graph\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os, warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def tic(section):\n",
    "    print(f\"\\n>>> Starting: {section}\")\n",
    "    return time.perf_counter()\n",
    "\n",
    "def toc(t0, section):\n",
    "    dt = time.perf_counter() - t0\n",
    "    print(f\"completed: {section} in {dt:.2f}s\")\n",
    "\n",
    "#  Shared evaluation sample (deterministic)\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "if EVAL_ON_SAMPLE:\n",
    "    EVAL_IDX = rng.choice(X_model_kmeans.shape[0], size=min(EVAL_SAMPLE_SIZE, X_model_kmeans.shape[0]), replace=False)\n",
    "else:\n",
    "    EVAL_IDX = np.arange(X_model_kmeans.shape[0])\n",
    "\n",
    "def _eval_scores(X, y, metric=\"euclidean\"):\n",
    "    \"\"\"Internal scores on the shared evaluation sample; ignores noise (-1).\"\"\"\n",
    "    y_eval = y[EVAL_IDX]\n",
    "    mask = (y_eval != NOISE_LABEL) if (NOISE_LABEL in y_eval) else np.ones_like(y_eval, bool)\n",
    "    if mask.sum() < 50 or len(np.unique(y_eval[mask])) < 2:\n",
    "        return {\"sil\": np.nan, \"ch\": np.nan, \"db\": np.nan}\n",
    "    Xs, ys = X[EVAL_IDX][mask], y_eval[mask]\n",
    "    out = {\"sil\": np.nan, \"ch\": np.nan, \"db\": np.nan}\n",
    "    if COMPUTE_SILHOUETTE:\n",
    "        out[\"sil\"] = float(silhouette_score(Xs, ys, metric=metric))\n",
    "    if COMPUTE_CH_DB:\n",
    "        out[\"ch\"]  = float(calinski_harabasz_score(Xs, ys))\n",
    "        out[\"db\"]  = float(davies_bouldin_score(Xs, ys))\n",
    "    return out\n",
    "\n",
    "labels_dict = {}\n",
    "score_rows  = []\n",
    "\n",
    "#  1) MiniBatch K-Means (on cosine-like L2 space)\n",
    "t0 = tic(\"MiniBatch K-Means\")\n",
    "best_km, best_k, best_sil = None, None, -np.inf\n",
    "for k in KMEANS_K_GRID:\n",
    "    km = MiniBatchKMeans(\n",
    "        n_clusters=k,\n",
    "        batch_size=KMEANS_BATCH_SIZE,\n",
    "        max_iter=KMEANS_MAX_ITER,\n",
    "        n_init=KMEANS_N_INIT,\n",
    "        random_state=RANDOM_SEED,\n",
    "        init=\"k-means++\"\n",
    "    )\n",
    "    km.fit(X_model_kmeans)\n",
    "    labs = km.predict(X_model_kmeans)\n",
    "    labels_dict[f\"kmeans_{k}\"] = labs\n",
    "    s = _eval_scores(X_model_kmeans, labs)[\"sil\"]\n",
    "    if np.nan_to_num(s, nan=-1) > best_sil:\n",
    "        best_sil, best_km, best_k = s, km, k\n",
    "print(\"K-Means tried:\", KMEANS_K_GRID, \"| best k:\", best_k, \"| silhouette(eval):\", None if np.isnan(best_sil) else round(best_sil, 4))\n",
    "toc(t0, \"MiniBatch K-Means\")\n",
    "\n",
    "#  2) GMM (choose K by BIC on PCA space)\n",
    "t0 = tic(\"Gaussian Mixture Models (BIC)\")\n",
    "best_gmm, best_bic, best_g = None, np.inf, None\n",
    "for g in GMM_K_GRID:\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=g,\n",
    "        covariance_type=GMM_COV_TYPE,\n",
    "        random_state=GMM_RANDOM_STATE,\n",
    "        max_iter=GMM_MAX_ITER,\n",
    "        n_init=GMM_N_INIT,\n",
    "        reg_covar=1e-6,\n",
    "        init_params=\"kmeans\"\n",
    "    )\n",
    "    gmm.fit(X_model_gmm)\n",
    "    bic = gmm.bic(X_model_gmm)\n",
    "    labels_dict[f\"gmm_{g}\"] = gmm.predict(X_model_gmm)\n",
    "    if bic < best_bic:\n",
    "        best_bic, best_g, best_gmm = bic, g, gmm\n",
    "print(\"GMM best k by BIC:\", best_g, \"| BIC:\", round(best_bic, 2))\n",
    "toc(t0, \"Gaussian Mixture Models (BIC)\")\n",
    "\n",
    "# 3) HDBSCAN (fit on sample if configured) + approximate_predict\n",
    "t0 = tic(\"HDBSCAN + propagation\")\n",
    "n = X_model_hdbscan.shape[0]\n",
    "fit_n = min(HDBSCAN_SAMPLE_SIZE, n) if USE_SAMPLED_CLUSTERING else n\n",
    "fit_idx = (rng.choice(n, size=fit_n, replace=False) if fit_n < n else np.arange(n))\n",
    "\n",
    "print(f\"[HDBSCAN] fit_size={fit_idx.size:,}/{n:,} | min_cluster_size={HDBSCAN_MIN_CLUSTER_SIZE} | min_samples={HDBSCAN_MIN_SAMPLES} | metric={HDBSCAN_METRIC} | selection={HDBSCAN_CLUSTER_SELECTION}\")\n",
    "hdb = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE,\n",
    "    min_samples=HDBSCAN_MIN_SAMPLES,\n",
    "    metric=HDBSCAN_METRIC,\n",
    "    cluster_selection_method=HDBSCAN_CLUSTER_SELECTION,\n",
    "    cluster_selection_epsilon=HDBSCAN_SELECTION_EPSILON,\n",
    "    allow_single_cluster=HDBSCAN_ALLOW_SINGLE_CLUSTER,\n",
    "    core_dist_n_jobs=(os.cpu_count() or 1),\n",
    "    prediction_data=True\n",
    ").fit(X_model_hdbscan[fit_idx])\n",
    "\n",
    "from hdbscan import prediction as hdb_pred\n",
    "if fit_idx.size < n and HDBSCAN_APPROX_PREDICT:\n",
    "    labs_full, strengths = hdb_pred.approximate_predict(hdb, X_model_hdbscan)\n",
    "    labs_full = labs_full.astype(int)\n",
    "else:\n",
    "    labs_full = hdb.labels_.astype(int)\n",
    "    strengths = getattr(hdb, \"probabilities_\", np.ones_like(labs_full, dtype=float))\n",
    "\n",
    "# optional confidence thresholding\n",
    "if HDBSCAN_CLUSTER_PROB_THRESH > 0.0:\n",
    "    low_conf = strengths < HDBSCAN_CLUSTER_PROB_THRESH\n",
    "    labs_full = labs_full.copy()\n",
    "    labs_full[low_conf] = NOISE_LABEL\n",
    "\n",
    "labels_dict[\"hdbscan\"] = labs_full\n",
    "labels_dict[\"hdbscan_conf\"] = strengths\n",
    "hdb_scores = _eval_scores(X_model_hdbscan, labs_full, metric=(\"cosine\" if HDBSCAN_METRIC == \"cosine\" else \"euclidean\"))\n",
    "print(\"HDBSCAN clusters (excl. noise):\", len(np.unique(labs_full[labs_full != NOISE_LABEL])))\n",
    "print(\"HDBSCAN eval:\", hdb_scores)\n",
    "toc(t0, \"HDBSCAN + propagation\")\n",
    "\n",
    "# 4) Agglomerative (Ward) + kNN propagation (kept as in prior runs)\n",
    "if USE_AGGLO:\n",
    "    t0 = tic(\"Agglomerative (Ward) + kNN propagation\")\n",
    "    # Fit on sample if sampling enabled\n",
    "    agg_fit_n = min( max(AGGLOMERATIVE_NCL * 4_000, 60_000), X_model_kmeans.shape[0]) if USE_SAMPLED_CLUSTERING else X_model_kmeans.shape[0]\n",
    "    agg_idx = (rng.choice(X_model_kmeans.shape[0], size=agg_fit_n, replace=False)\n",
    "               if agg_fit_n < X_model_kmeans.shape[0] else np.arange(X_model_kmeans.shape[0]))\n",
    "    X_agg_fit = X_model_kmeans[agg_idx]\n",
    "\n",
    "    print(f\"[Agglo] building connectivity graph (n={X_agg_fit.shape[0]:,}, k={K_CONNECTIVITY})\")\n",
    "    graph_conn = kneighbors_graph(X_agg_fit, n_neighbors=K_CONNECTIVITY, mode=\"connectivity\", metric=\"euclidean\")\n",
    "\n",
    "    print(f\"[Agglo] fitting Ward linkage with n_clusters={AGGLOMERATIVE_NCL}\")\n",
    "    agg_ward = AgglomerativeClustering(n_clusters=AGGLOMERATIVE_NCL, linkage=\"ward\", connectivity=graph_conn)\n",
    "    labels_agg_fit = agg_ward.fit_predict(X_agg_fit)\n",
    "\n",
    "    print(\"Agglomerative: propagating labels to full dataset via kNN\")\n",
    "    knn_ag = KNeighborsClassifier(n_neighbors=K_PROPAGATE, weights=\"distance\")\n",
    "    knn_ag.fit(X_agg_fit, labels_agg_fit)\n",
    "    labels_agg_full = knn_ag.predict(X_model_kmeans).astype(int)\n",
    "    labels_dict[f\"agg_ward_{AGGLOMERATIVE_NCL}\"] = labels_agg_full\n",
    "\n",
    "    agg_scores = _eval_scores(X_model_kmeans, labels_agg_full, metric=\"euclidean\")\n",
    "    print(\"Agglomerative clusters:\", len(np.unique(labels_agg_full)))\n",
    "    print(\"Agglomerative eval:\", agg_scores)\n",
    "    toc(t0, \"Agglomerative (Ward) + kNN propagation\")\n",
    "\n",
    "# 5) DBSCAN (k-distance eps estimation on sample) + optional propagation\n",
    "if USE_DBSCAN:\n",
    "    t0 = tic(\"DBSCAN (eps via k-distance) + optional propagation\")\n",
    "\n",
    "    # choose sample for eps estimation (and for fit if sampling)\n",
    "    eps_idx = (rng.choice(X_model_dbscan.shape[0], size=min(DBSCAN_EPS_SAMPLE_N, X_model_dbscan.shape[0]), replace=False)\n",
    "               if USE_SAMPLED_CLUSTERING else np.arange(X_model_dbscan.shape[0]))\n",
    "    X_db_eps = X_model_dbscan[eps_idx]\n",
    "\n",
    "    print(f\"[DBSCAN] estimating eps via k-distance on {X_db_eps.shape[0]:,} points (k={DBSCAN_MIN_SAMPLES}, metric={DBSCAN_METRIC})\")\n",
    "    nn = NearestNeighbors(n_neighbors=DBSCAN_MIN_SAMPLES, metric=DBSCAN_METRIC).fit(X_db_eps)\n",
    "    dists, _ = nn.kneighbors(X_db_eps)\n",
    "    kdist = np.sort(dists[:, -1])\n",
    "    eps_grid = [float(np.percentile(kdist, p)) for p in [95, 96, 97, 98, 99]]\n",
    "\n",
    "    best_eps, best_s = None, -np.inf\n",
    "    for eps in eps_grid:\n",
    "        tmp_labels = SK_DBSCAN(eps=eps, min_samples=DBSCAN_MIN_SAMPLES, metric=DBSCAN_METRIC).fit_predict(X_db_eps)\n",
    "        # evaluate fairly on same eval index — propagate if sample != full\n",
    "        if X_db_eps.shape[0] == X_model_dbscan.shape[0]:\n",
    "            s = _eval_scores(X_model_dbscan, tmp_labels, metric=(\"cosine\" if DBSCAN_METRIC == \"cosine\" else \"euclidean\"))[\"sil\"]\n",
    "        else:\n",
    "            mask_fit = (tmp_labels != NOISE_LABEL)\n",
    "            if mask_fit.sum() > 0:\n",
    "                knn_db = KNeighborsClassifier(n_neighbors=K_PROPAGATE, weights=\"distance\").fit(X_db_eps[mask_fit], tmp_labels[mask_fit])\n",
    "                lab_full_tmp = np.full(X_model_dbscan.shape[0], NOISE_LABEL, dtype=int)\n",
    "                lab_full_tmp[eps_idx] = tmp_labels\n",
    "                rest_idx = np.setdiff1d(np.arange(X_model_dbscan.shape[0]), eps_idx, assume_unique=True)\n",
    "                lab_full_tmp[rest_idx] = knn_db.predict(X_model_dbscan[rest_idx])\n",
    "                s = _eval_scores(X_model_dbscan, lab_full_tmp, metric=(\"cosine\" if DBSCAN_METRIC == \"cosine\" else \"euclidean\"))[\"sil\"]\n",
    "            else:\n",
    "                s = -np.inf\n",
    "        if np.nan_to_num(s, nan=-1) > best_s:\n",
    "            best_s, best_eps = s, eps\n",
    "\n",
    "    print(\"DBSCAN chosen eps:\", round(best_eps, 5), \"| silhouette(eval):\", None if np.isnan(best_s) else round(best_s, 4))\n",
    "\n",
    "    # final fit (sample + propagate, or full)\n",
    "    if USE_SAMPLED_CLUSTERING:\n",
    "        fit_idx = (rng.choice(X_model_dbscan.shape[0], size=min(DBSCAN_FIT_SAMPLE_N, X_model_dbscan.shape[0]), replace=False))\n",
    "        X_db_fit = X_model_dbscan[fit_idx]\n",
    "        db = SK_DBSCAN(eps=best_eps, min_samples=DBSCAN_MIN_SAMPLES, metric=DBSCAN_METRIC).fit(X_db_fit)\n",
    "        lab_fit = db.labels_.astype(int)\n",
    "\n",
    "        print(\"DBSCAN: propagating labels to full dataset via kNN...\")\n",
    "        labels_db_full = np.full(X_model_dbscan.shape[0], NOISE_LABEL, dtype=int)\n",
    "        labels_db_full[fit_idx] = lab_fit\n",
    "        mask_fit = (lab_fit != NOISE_LABEL)\n",
    "        if mask_fit.sum() > 0:\n",
    "            knn = KNeighborsClassifier(n_neighbors=K_PROPAGATE, weights=\"distance\").fit(X_db_fit[mask_fit], lab_fit[mask_fit])\n",
    "            rest_idx = np.setdiff1d(np.arange(X_model_dbscan.shape[0]), fit_idx, assume_unique=True)\n",
    "            labels_db_full[rest_idx] = knn.predict(X_model_dbscan[rest_idx])\n",
    "    else:\n",
    "        db = SK_DBSCAN(eps=best_eps, min_samples=DBSCAN_MIN_SAMPLES, metric=DBSCAN_METRIC).fit(X_model_dbscan)\n",
    "        labels_db_full = db.labels_.astype(int)\n",
    "\n",
    "    db_key = f\"dbscan_eps{round(best_eps,5)}_ms{DBSCAN_MIN_SAMPLES}\"\n",
    "    labels_dict[db_key] = labels_db_full\n",
    "    db_scores = _eval_scores(X_model_dbscan, labels_db_full, metric=(\"cosine\" if DBSCAN_METRIC == \"cosine\" else \"euclidean\"))\n",
    "    print(\"DBSCAN clusters (excl. noise):\", len(np.unique(labels_db_full[labels_db_full != NOISE_LABEL])))\n",
    "    print(\"DBSCAN eval:\", db_scores)\n",
    "    toc(t0, \"DBSCAN (eps via k-distance) + optional propagation\")\n",
    "\n",
    "# 6) Internal score summary + save\n",
    "t0 = tic(\"Score summary + save\")\n",
    "\n",
    "def _n_clusters(y):\n",
    "    u = np.unique(y)\n",
    "    return int(len(u) - (1 if NOISE_LABEL in u else 0))\n",
    "\n",
    "rows = []\n",
    "rows.append((\"kmeans_\" + str(best_k),) + tuple(_eval_scores(X_model_kmeans, labels_dict[f\"kmeans_{best_k}\"]).values()))\n",
    "rows.append((\"gmm_\" + str(best_g),) + tuple(_eval_scores(X_model_gmm,    labels_dict[f\"gmm_{best_g}\"]).values()))\n",
    "rows.append((\"hdbscan\",)            + tuple(_eval_scores(X_model_hdbscan, labels_dict[\"hdbscan\"], metric=(\"cosine\" if HDBSCAN_METRIC==\"cosine\" else \"euclidean\")).values()))\n",
    "if USE_AGGLO:\n",
    "    rows.append((f\"agg_ward_{AGGLOMERATIVE_NCL}\",) + tuple(_eval_scores(X_model_kmeans, labels_dict[f\"agg_ward_{AGGLOMERATIVE_NCL}\"]).values()))\n",
    "if USE_DBSCAN:\n",
    "    rows.append((db_key,) + tuple(_eval_scores(X_model_dbscan, labels_dict[db_key], metric=(\"cosine\" if DBSCAN_METRIC==\"cosine\" else \"euclidean\")).values()))\n",
    "\n",
    "score_df = pd.DataFrame(rows, columns=[\"model\", \"silhouette\", \"ch\", \"db\"])\n",
    "# add #clusters and noise%\n",
    "def _noise_pct(name):\n",
    "    if name == \"hdbscan\":\n",
    "        return float((labels_dict[\"hdbscan\"] == NOISE_LABEL).mean())\n",
    "    if name.startswith(\"dbscan_\"):\n",
    "        return float((labels_dict[name] == NOISE_LABEL).mean())\n",
    "    return 0.0\n",
    "\n",
    "score_df[\"n_clusters\"] = score_df[\"model\"].map(lambda m: _n_clusters(labels_dict[\"hdbscan\"]) if m==\"hdbscan\"\n",
    "                                               else (_n_clusters(labels_dict[db_key]) if m.startswith(\"dbscan_\")\n",
    "                                                     else _n_clusters(labels_dict[m])))\n",
    "score_df[\"noise_pct\"]  = score_df[\"model\"].map(_noise_pct)\n",
    "\n",
    "score_df.to_csv(SCORECARD_OUT, index=False)\n",
    "print(\"Saved scorecard ->\", SCORECARD_OUT)\n",
    "print(score_df)\n",
    "toc(t0, \"Score summary + save\")\n",
    "\n",
    "# 7) Save full labels (all models)\n",
    "t0 = tic(\"Saving labels (full)\")\n",
    "labels_df = pd.DataFrame({\"session_id\": session_to_items.index.to_numpy()})\n",
    "for name, lab in labels_dict.items():\n",
    "    if name.endswith(\"_conf\"):\n",
    "        continue\n",
    "    labels_df[name] = lab\n",
    "if \"hdbscan_conf\" in labels_dict:\n",
    "    labels_df[\"hdbscan_conf\"] = labels_dict[\"hdbscan_conf\"]\n",
    "\n",
    "LABELS_OUT = run_stem(\"cluster_labels_all_models.csv\")\n",
    "labels_df.to_csv(LABELS_OUT, index=False)\n",
    "print(\"Saved labels ->\", LABELS_OUT)\n",
    "print(\"Clusterers saved:\", list(labels_dict.keys()))\n",
    "toc(t0, \"Saving labels (full)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37075e2-63b6-47c2-b384-14f4b39be4f6",
   "metadata": {},
   "source": [
    "## 5. Baseline Static Model Implementation RFM Inspired\n",
    "\n",
    "### 5.1. Session level RFM-style feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42edd05f-5612-41e2-a926-590ccf01f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Session feature extraction (fast path)\n",
      ">>> cast dtypes\n",
      "---- cast dtypes in 0.94s\n",
      ">>> event counts (crosstab)\n",
      "---- event counts done in 0.35s\n",
      ">>> timings (min/max/duration)\n",
      "---- timings done in 0.08s\n",
      ">>> unique items (item_diversity)\n",
      "---- unique items done in 0.09s\n",
      ">>> last event via idxmax\n",
      "---- last event done in 0.09s\n",
      ">>> assembling features\n",
      "---- features assembled in 0.19s\n",
      "---- Session feature extraction in 1.73s\n",
      "Session-level feature table saved to: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_session_features_rfm_proxies.csv\n",
      "time: 4.66 s (started: 2025-08-27 18:18:37 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# 5.1) Feature extraction for session-level RFM proxies\n",
    "\n",
    "# 1) Imports\n",
    "import time, numpy as np, pandas as pd, math\n",
    "\n",
    "def tic(msg):\n",
    "    print(f\">>> {msg}\"); return time.perf_counter()\n",
    "def toc(t0, msg):\n",
    "    print(f\"---- {msg} in {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "assert {\"session_id\",\"timestamp\",\"event\",\"itemid\"}.issubset(events.columns)\n",
    "\n",
    "t0_all = tic(\"Session feature extraction (fast path)\")\n",
    "\n",
    "# 2) dtypes and normalisation (optimises groupby performance)\n",
    "t0 = tic(\"cast dtypes\")\n",
    "ev = events[[\"session_id\",\"timestamp\",\"event\",\"itemid\"]].copy()\n",
    "ev[\"event\"] = ev[\"event\"].astype(str).str.lower().astype(\"category\")\n",
    "if ev[\"itemid\"].dtype != \"category\":\n",
    "    ev[\"itemid\"] = ev[\"itemid\"].astype(\"category\")\n",
    "# respect global timestamp unit (supports ms->s if TS_DIV=1000)\n",
    "ev[\"timestamp\"] = (ev[\"timestamp\"].astype(\"int64\") // TS_DIV).astype(\"int64\")\n",
    "toc(t0, \"cast dtypes\")\n",
    "\n",
    "# 3) Vectorised counts per (session,event)\n",
    "t0 = tic(\"event counts (crosstab)\")\n",
    "counts = (ev.groupby([\"session_id\",\"event\"], observed=True)\n",
    "            .size()\n",
    "            .unstack(fill_value=0))\n",
    "# ensure expected columns exist (singular names to match PROFILE_FEATURES)\n",
    "for col in [\"view\",\"addtocart\",\"transaction\"]:\n",
    "    if col not in counts.columns:\n",
    "        counts[col] = 0\n",
    "# keep singular column names\n",
    "counts = counts[[\"view\",\"addtocart\",\"transaction\"]].astype(\"int32\")\n",
    "counts[\"events\"] = counts.sum(axis=1).astype(\"int32\")\n",
    "toc(t0, \"event counts done\")\n",
    "\n",
    "# 4) Timing statistics (min/max/duration) without sorting\n",
    "t0 = tic(\"timings (min/max/duration)\")\n",
    "g = ev.groupby(\"session_id\", observed=True, sort=False)\n",
    "t_min = g[\"timestamp\"].min()\n",
    "t_max = g[\"timestamp\"].max()\n",
    "duration_sec = (t_max - t_min).astype(\"float64\").clip(lower=0.0)\n",
    "toc(t0, \"timings done\")\n",
    "\n",
    "# 5) Unique items per session (item_diversity)\n",
    "t0 = tic(\"unique items (item_diversity)\")\n",
    "item_diversity = g[\"itemid\"].nunique().astype(\"int32\")\n",
    "toc(t0, \"unique items done\")\n",
    "\n",
    "# 6) Last event per session -> one-hot\n",
    "t0 = tic(\"last event via idxmax\")\n",
    "idx_last = g[\"timestamp\"].idxmax()\n",
    "last_event = ev.loc[idx_last, [\"session_id\",\"event\"]].set_index(\"session_id\")[\"event\"]\n",
    "last_dummies = pd.get_dummies(last_event, prefix=\"last_evt\", dtype=\"int8\")\n",
    "for col in [\"last_evt_view\",\"last_evt_addtocart\",\"last_evt_transaction\"]:\n",
    "    if col not in last_dummies.columns:\n",
    "        last_dummies[col] = 0\n",
    "toc(t0, \"last event done\")\n",
    "\n",
    "# 7) Assemble features (rates, time-of-day, recency, bounce)\n",
    "t0 = tic(\"assembling features\")\n",
    "sess = pd.DataFrame({\n",
    "    \"duration_sec\": duration_sec,\n",
    "    \"item_diversity\": item_diversity,\n",
    "}).join(counts, how=\"left\").join(last_dummies, how=\"left\")\n",
    "\n",
    "# rates/ratios\n",
    "sess[\"events_per_min\"] = (sess[\"events\"] / np.maximum(sess[\"duration_sec\"]/60.0, 1e-6)).astype(\"float64\")\n",
    "\n",
    "# time-of-day + day-of-week from session start\n",
    "hour = pd.to_datetime(t_min, unit=\"s\", errors=\"coerce\").dt.hour.fillna(0).astype(\"int16\")\n",
    "dow  = pd.to_datetime(t_min, unit=\"s\", errors=\"coerce\").dt.dayofweek.fillna(0).astype(\"int8\")\n",
    "sess[\"hour\"] = hour\n",
    "sess[\"dow\"]  = dow\n",
    "sess[\"is_evening\"] = ((hour >= 18) & (hour <= 23)).astype(\"int8\")\n",
    "\n",
    "# recency (days from dataset end to session end)\n",
    "dataset_end = ev[\"timestamp\"].max()\n",
    "sess[\"recency_days\"] = ((dataset_end - t_max) / (60*60*24)).astype(\"float64\")\n",
    "\n",
    "# bounce flag\n",
    "sess[\"is_bounce\"] = (sess[\"events\"] <= 2).astype(\"int8\")\n",
    "\n",
    "# clean index name\n",
    "sess.index.name = \"session_id\"\n",
    "toc(t0, \"features assembled\")\n",
    "toc(t0_all, \"Session feature extraction\")\n",
    "\n",
    "# persist features for downstream SHAP/personas/reporting\n",
    "out_feat = run_stem(\"session_features_rfm_proxies.csv\")\n",
    "sess.to_csv(out_feat)\n",
    "print(\"Session-level feature table saved to:\", out_feat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1672e03-4f66-4f58-acef-ecdbffba49a7",
   "metadata": {},
   "source": [
    "### 5.2. Session-level RFM-proxy model (K-Means & profiling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e58b38-450e-4f9f-b4dd-c3de2350c88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> RFM-proxy model — data preprocessing checks\n",
      "---- pre-checks complete in 0.00s\n",
      "\n",
      ">>> Constructing R/F/M proxies\n",
      "Non-zero rates -> tx: 3.1609%, atc: 10.0451%\n",
      "Chosen M proxy: M_tx_plus_atc_log1p\n",
      "RFM-proxy preview:\n",
      "            R_recency_days  F_events_log1p  M_intent_log1p\n",
      "session_id                                                \n",
      "1                 0.006262        1.386294             0.0\n",
      "3                 0.041366        2.197225             0.0\n",
      "8                 0.018843        1.609438             0.0\n",
      "10                0.126887        1.098612             0.0\n",
      "17                0.091933        1.098612             0.0\n",
      "---- R/F/M proxies ready in 0.09s\n",
      "\n",
      ">>> Clustering RFM-proxy with K-Means + silhouette(K grid)\n",
      "k=4  -> silhouette(sample)=0.4175\n",
      "k=6  -> silhouette(sample)=0.4656\n",
      "k=8  -> silhouette(sample)=0.4175\n",
      "k=10 -> silhouette(sample)=0.4405\n",
      "k=12 -> silhouette(sample)=0.4465\n",
      "Chosen K = 6 (silhouette=0.4656)\n",
      "---- K-Means clustering completed for RFM Baseline model in 23.37s\n",
      "\n",
      ">>> Profiling clusters (size, share, quantiles, intent rate)\n",
      "[RFM-proxy] Full-data silhouette at K=6: 0.4614\n",
      "            n  share  tx_rate  atc_rate  R_p25  R_median  R_p75  F_p25  F_median  F_p75  M_p25  M_median  M_p75\n",
      "label                                                                                                          \n",
      "0       41761  0.113    0.000     0.011  0.050     0.079  0.108  1.792     1.946  2.303  0.000     0.000  0.000\n",
      "1      145960  0.393    0.000     0.000  0.087     0.105  0.121  1.099     1.099  1.386  0.000     0.000  0.000\n",
      "2        3030  0.008    0.673     0.959  0.037     0.069  0.104  2.197     2.485  2.944  1.386     1.386  1.705\n",
      "3      144785  0.390    0.000     0.000  0.018     0.037  0.054  1.099     1.099  1.386  0.000     0.000  0.000\n",
      "4       20639  0.056    0.000     1.000  0.039     0.071  0.104  1.099     1.386  1.609  0.405     0.405  0.405\n",
      "5       14920  0.040    0.650     0.890  0.036     0.069  0.103  1.386     1.609  1.946  0.693     0.916  0.916\n",
      "---- profiling completed in 2234.62s\n",
      "\n",
      ">>> Saving results\n",
      "Saved features -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_session_rfm_proxy_features.parquet\n",
      "Saved labels  -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_session_rfm_proxy_labels_k6.csv\n",
      "Saved profile -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_session_rfm_proxy_profile_k6.csv\n",
      "---- results saved in 0.68s\n",
      "time: 37min 38s (started: 2025-08-27 18:18:42 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# 5.2) Session-level RFM-proxy model (K-Means & profiling)\n",
    "\n",
    "\n",
    "# 1) Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import time\n",
    "\n",
    "# pulling from config\n",
    "RANDOM_SEED  = globals().get(\"RANDOM_SEED\", 42)\n",
    "K_GRID_BASE  = globals().get(\"K_GRID_BASE\", [2, 4, 6, 8, 10, 12])\n",
    "\n",
    "# tiny timers\n",
    "def tic(msg): print(f\"\\n>>> {msg}\"); return time.perf_counter()\n",
    "def toc(t0, msg): print(f\"---- {msg} in {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "# Preconditions\n",
    "t0 = tic(\"RFM-proxy model — data preprocessing checks\")\n",
    "assert \"sess\" in globals(), \"Expected 'sess' from the fast extractor.\"\n",
    "need = {\"recency_days\",\"events\",\"addtocart\",\"transaction\"}   # singular names per extractor\n",
    "missing = need - set(sess.columns)\n",
    "assert not missing, f\"'sess' is missing columns: {missing}\"\n",
    "toc(t0, \"pre-checks complete\")\n",
    "\n",
    "# 2) Construct R/F/M proxies\n",
    "t0 = tic(\"Constructing R/F/M proxies\")\n",
    "\n",
    "R_proxy = sess[\"recency_days\"].astype(\"float64\")\n",
    "F_proxy = np.log1p(sess[\"events\"].astype(\"float64\"))\n",
    "\n",
    "tx   = sess[\"transaction\"].astype(\"float64\")\n",
    "atc  = sess[\"addtocart\"].astype(\"float64\")\n",
    "views = (sess[\"events\"].astype(\"float64\") - atc - tx).clip(lower=0)  # crude proxy\n",
    "\n",
    "print(f\"Non-zero rates -> tx: {(tx>0).mean():.4%}, atc: {(atc>0).mean():.4%}\")\n",
    "\n",
    "M_candidates = {\n",
    "    \"M_tx_plus_atc_log1p\": np.log1p(tx + ATC_WEIGHT * atc),         # recommended\n",
    "    \"M_tx_log1p\":          np.log1p(tx),                             # strict monetary\n",
    "    \"M_intent_per_view\":   np.log1p((tx + ATC_WEIGHT * atc) / (1.0 + views))\n",
    "}\n",
    "\n",
    "chosen_M_name, M_proxy = None, None\n",
    "for name, arr in M_candidates.items():\n",
    "    if np.nanstd(arr) > 0:\n",
    "        chosen_M_name, M_proxy = name, arr\n",
    "        break\n",
    "\n",
    "if M_proxy is None:\n",
    "    print(\"All M candidates are constant -> falling back to RF baseline.\")\n",
    "    rfm_proxy = pd.DataFrame({\n",
    "        \"R_recency_days\": R_proxy,\n",
    "        \"F_events_log1p\": F_proxy,\n",
    "    }, index=sess.index)\n",
    "else:\n",
    "    print(f\"Chosen M proxy: {chosen_M_name}\")\n",
    "    rfm_proxy = pd.DataFrame({\n",
    "        \"R_recency_days\": R_proxy,\n",
    "        \"F_events_log1p\": F_proxy,\n",
    "        \"M_intent_log1p\": M_proxy,\n",
    "    }, index=sess.index)\n",
    "\n",
    "# Scaling\n",
    "USE_ROBUST = True  # set False to use StandardScaler\n",
    "scaler = RobustScaler(quantile_range=(5, 95)) if USE_ROBUST else StandardScaler()\n",
    "X_rfm = scaler.fit_transform(rfm_proxy.values).astype(\"float32\", copy=False)\n",
    "\n",
    "print(\"RFM-proxy preview:\")\n",
    "print(rfm_proxy.head().to_string())\n",
    "toc(t0, \"R/F/M proxies ready\")\n",
    "\n",
    "# 3) K-Means with sampled silhouette over the K grid\n",
    "t0 = tic(\"Clustering RFM-proxy with K-Means + silhouette(K grid)\")\n",
    "n = X_rfm.shape[0]\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "idx_sil = rng.choice(n, size=min(SIL_SAMPLE_N, n), replace=False)\n",
    "\n",
    "best_k, best_sil, best_labels = None, -1.0, None\n",
    "labels_by_k = {}\n",
    "\n",
    "for k in K_GRID_BASE:\n",
    "    if k < 2 or k >= n:\n",
    "        continue\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=RANDOM_SEED)\n",
    "    y  = km.fit_predict(X_rfm)\n",
    "    labels_by_k[k] = y\n",
    "    sil = silhouette_score(X_rfm[idx_sil], y[idx_sil], metric=\"euclidean\")\n",
    "    print(f\"k={k:<2d} -> silhouette(sample)={sil:.4f}\")\n",
    "    if np.isfinite(sil) and sil > best_sil:\n",
    "        best_k, best_sil, best_labels = k, sil, y\n",
    "\n",
    "if best_k is None:\n",
    "    raise RuntimeError(\"No valid K found. Please adjust K_GRID_BASE.\")\n",
    "\n",
    "print(f\"Chosen K = {best_k} (silhouette={best_sil:.4f})\")\n",
    "toc(t0, \"K-Means clustering completed for RFM Baseline model\")\n",
    "\n",
    "# 4) Profiling\n",
    "t0 = tic(\"Profiling clusters (size, share, quantiles, intent rate)\")\n",
    "full_sil = float(silhouette_score(X_rfm, best_labels, metric=\"euclidean\"))\n",
    "print(f\"[RFM-proxy] Full-data silhouette at K={best_k}: {full_sil:.4f}\")\n",
    "\n",
    "prof = rfm_proxy.copy()\n",
    "prof[\"label\"] = best_labels\n",
    "prof[\"tx\"]    = (sess[\"transaction\"] > 0).astype(int)\n",
    "prof[\"atc\"]   = (sess[\"addtocart\"] > 0).astype(int)\n",
    "\n",
    "g = prof.groupby(\"label\", observed=True)\n",
    "cluster_size = g.size().rename(\"n\")\n",
    "share = (cluster_size / len(prof)).rename(\"share\")\n",
    "tx_rate  = g[\"tx\"].mean().rename(\"tx_rate\")\n",
    "atc_rate = g[\"atc\"].mean().rename(\"atc_rate\")\n",
    "\n",
    "def q(x):\n",
    "    return pd.Series({\"p25\": np.percentile(x,25), \"median\": np.median(x), \"p75\": np.percentile(x,75)})\n",
    "\n",
    "desc_parts = [g[\"R_recency_days\"].apply(q).unstack().add_prefix(\"R_\"),\n",
    "              g[\"F_events_log1p\"].apply(q).unstack().add_prefix(\"F_\")]\n",
    "if \"M_intent_log1p\" in prof.columns:\n",
    "    desc_parts.append(g[\"M_intent_log1p\"].apply(q).unstack().add_prefix(\"M_\"))\n",
    "\n",
    "desc = pd.concat(desc_parts, axis=1)\n",
    "summary = pd.concat([cluster_size, share, tx_rate, atc_rate], axis=1).join(desc)\n",
    "print(summary.round(3).to_string())\n",
    "toc(t0, \"profiling completed\")\n",
    "\n",
    "# 5) Saving results\n",
    "t0 = tic(\"Saving results\")\n",
    "\n",
    "out = sess.join(rfm_proxy)\n",
    "out[\"rfm_proxy_k\"] = best_k\n",
    "out[\"rfm_proxy_label\"] = best_labels\n",
    "out[\"rfm_full_silhouette\"] = full_sil\n",
    "out[\"rfm_chosen_M\"] = chosen_M_name if chosen_M_name is not None else \"none\"\n",
    "out[\"rfm_scaler\"] = \"Robust(5-95)\" if isinstance(scaler, RobustScaler) else \"Standard\"\n",
    "\n",
    "feat_path = run_stem(\"session_rfm_proxy_features.parquet\")\n",
    "lab_path  = run_stem(f\"session_rfm_proxy_labels_k{best_k}.csv\")\n",
    "prof_path = run_stem(f\"session_rfm_proxy_profile_k{best_k}.csv\")\n",
    "\n",
    "out.to_parquet(feat_path, index=True)\n",
    "summary.reset_index().to_csv(prof_path, index=False)\n",
    "pd.DataFrame({\"session_id\": out.index, \"label\": best_labels}).to_csv(lab_path, index=False)\n",
    "\n",
    "print(f\"Saved features -> {feat_path}\")\n",
    "print(f\"Saved labels  -> {lab_path}\")\n",
    "print(f\"Saved profile -> {prof_path}\")\n",
    "\n",
    "# expose labels for downstream agreement/SHAP steps\n",
    "if \"labels_dict\" not in globals():\n",
    "    labels_dict = {}\n",
    "labels_dict[f\"rfm_proxy_kmeans_session_{best_k}\"] = best_labels\n",
    "\n",
    "toc(t0, \"results saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f8654-ec33-46dd-acd4-e85f108c5f60",
   "metadata": {},
   "source": [
    "## 6. Visualising clusters across all algorithms (using UMAP 2D overlays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d624d44-c9c7-43e7-87bb-b1041dfab898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> UMAP overlays — prechecks\n",
      "---- prechecks completed in 0.00s\n",
      "\n",
      ">>> UMAP base (neural-embedding space): sample-fit + transform\n",
      "Reused precomputed umap_2d.\n",
      "Saved base UMAP coords to: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_umap_all_models_coords.parquet and C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_umap_all_models_coords.csv\n",
      "Note: UMAP is for visual inspection only and not a clustering tool.\n",
      "---- UMAP base ready in 1.26s\n",
      "\n",
      ">>> Plotting neural-embedding overlays for all models\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_2.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_3.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_4.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_6.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_8.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_10.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_12.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_14.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_3.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_4.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_6.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_8.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_10.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_12.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_14.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_16.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_hdbscan.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_agg_ward_12.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_dbscan_eps14.11645_ms8.png\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_rfm_proxy_kmeans_session_6.png\n",
      "---- neural overlays completed in 25.02s\n",
      "\n",
      ">>> UMAP for session RFM-proxy (3 features): sample-fit + transform\n",
      "Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_rfm_proxy_kmeans_session_6.png\n",
      "---- RFM-proxy UMAP completed in 182.55s\n",
      "\n",
      "Generated plots:\n",
      "- KMeans k=2 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_2.png\n",
      "- KMeans k=3 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_3.png\n",
      "- KMeans k=4 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_4.png\n",
      "- KMeans k=6 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_6.png\n",
      "- KMeans k=8 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_8.png\n",
      "- KMeans k=10 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_10.png\n",
      "- KMeans k=12 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_12.png\n",
      "- KMeans k=14 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_kmeans_14.png\n",
      "- GMM k=3 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_3.png\n",
      "- GMM k=4 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_4.png\n",
      "- GMM k=6 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_6.png\n",
      "- GMM k=8 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_8.png\n",
      "- GMM k=10 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_10.png\n",
      "- GMM k=12 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_12.png\n",
      "- GMM k=14 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_14.png\n",
      "- GMM k=16 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_gmm_16.png\n",
      "- HDBSCAN -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_hdbscan.png\n",
      "- Agglomerative ward_12 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_agg_ward_12.png\n",
      "- DBSCAN eps14.11645_ms8 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_dbscan_eps14.11645_ms8.png\n",
      "- rfm_proxy_KMeans k=session_6 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_rfm_proxy_kmeans_session_6.png\n",
      "- rfm_proxy_kmeans_session_6 -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\figs\\20250827_144050_umap_rfm_proxy_kmeans_session_6.png\n",
      "time: 3min 28s (started: 2025-08-27 18:56:21 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# 6) Visualising clusters across all algorithms (UMAP overlays)\n",
    "\n",
    "\n",
    "# 1) Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings, time, os\n",
    "\n",
    "# 2) Progress helpers\n",
    "def tic(msg): print(f\"\\n>>> {msg}\"); return time.perf_counter()\n",
    "def toc(t0, msg): print(f\"---- {msg} in {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 3) Preconditions & defaults\n",
    "t0 = tic(\"UMAP overlays — prechecks\")\n",
    "assert \"labels_dict\" in globals() and isinstance(labels_dict, dict) and labels_dict, \"labels_dict is missing/empty.\"\n",
    "\n",
    "# pull-through from config\n",
    "RANDOM_SEED = globals().get(\"RANDOM_SEED\", 42)\n",
    "RUN_ID      = globals().get(\"RUN_ID\", \"run\")\n",
    "UMAP_N_NEIGHBORS = int(globals().get(\"UMAP_N_NEIGHBORS\", 30))\n",
    "UMAP_MIN_DIST    = float(globals().get(\"UMAP_MIN_DIST\", 0.05))\n",
    "UMAP_METRIC      = globals().get(\"UMAP_METRIC\", \"cosine\")\n",
    "\n",
    "# figure/output dirs\n",
    "FIGS_DIR = globals().get(\"FIGS_DIR\", Path(\"outputs/figs\"))\n",
    "OUT_DIR  = globals().get(\"OUT_DIR\", Path(\"outputs\"))\n",
    "FIGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# reproducible RNG\n",
    "RNG = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Base neural-embedding matrix (prefer L2-PCA for cosine-like geometry)\n",
    "if \"X_model_kmeans\" in globals():\n",
    "    BASE_X = X_model_kmeans\n",
    "elif \"X_pca_l2\" in globals():\n",
    "    BASE_X = X_pca_l2\n",
    "elif \"X_pca\" in globals():\n",
    "    BASE_X = X_pca\n",
    "else:\n",
    "    raise AssertionError(\"Could not find BASE_X (X_model_kmeans/X_pca_l2/X_pca). Run the PCA block again (section 3).\")\n",
    "N = BASE_X.shape[0]\n",
    "\n",
    "# Session index for alignment\n",
    "if \"SESSION_INDEX\" in globals():\n",
    "    SESSION_INDEX = SESSION_INDEX\n",
    "elif \"session_to_items\" in globals():\n",
    "    SESSION_INDEX = session_to_items.index.to_numpy()\n",
    "else:\n",
    "    SESSION_INDEX = (events[[\"session_id\"]].drop_duplicates()\n",
    "                     .astype({\"session_id\":\"int64\"}).values.ravel())\n",
    "\n",
    "# Local plotting/runtime knobs\n",
    "PLOT_MAX      = 100_000\n",
    "POINT_SIZE    = 1.8\n",
    "PLOT_DPI      = 140\n",
    "REUSE_COORDS  = True\n",
    "FIT_SAMPLE_N  = 120_000\n",
    "BATCH_SIZE    = 100_000\n",
    "LOW_MEMORY    = True\n",
    "toc(t0, \"prechecks completed\")\n",
    "\n",
    "# 4) Build or reuse UMAP for neural-embedding space\n",
    "t0 = tic(\"UMAP base (neural-embedding space): sample-fit + transform\")\n",
    "\n",
    "coords_all = None\n",
    "if REUSE_COORDS and (\"umap_2d\" in globals()) and (umap_2d is not None) and (getattr(umap_2d, \"shape\", (0,0))[0] == N):\n",
    "    coords_all = umap_2d.astype(np.float32, copy=False)\n",
    "    print(\"Reused precomputed umap_2d.\")\n",
    "else:\n",
    "    fit_idx = RNG.choice(N, size=min(FIT_SAMPLE_N, N), replace=False)\n",
    "    umap_model_all = umap.UMAP(\n",
    "        n_neighbors=UMAP_N_NEIGHBORS,\n",
    "        min_dist=UMAP_MIN_DIST,\n",
    "        n_components=2,\n",
    "        random_state=RANDOM_SEED,\n",
    "        metric=(\"euclidean\" if BASE_X is X_model_kmeans else UMAP_METRIC),  # L2-PCA -> euclidean ok\n",
    "        low_memory=LOW_MEMORY,\n",
    "    ).fit(BASE_X[fit_idx].astype(np.float32, copy=False))\n",
    "\n",
    "    coords_all = np.empty((N, 2), dtype=np.float32)\n",
    "    coords_all[fit_idx] = umap_model_all.embedding_.astype(np.float32, copy=False)\n",
    "\n",
    "    rest = np.setdiff1d(np.arange(N), fit_idx, assume_unique=True)\n",
    "    for s in range(0, len(rest), BATCH_SIZE):\n",
    "        b = rest[s:s+BATCH_SIZE]\n",
    "        coords_all[b] = umap_model_all.transform(BASE_X[b].astype(np.float32, copy=False)).astype(np.float32, copy=False)\n",
    "\n",
    "    # expose for later reuse\n",
    "    umap_2d = coords_all\n",
    "\n",
    "# Save coordinates (run-stamped under outputs/)\n",
    "coords_all_df = pd.DataFrame({\n",
    "    \"session_id\": SESSION_INDEX,\n",
    "    \"umap_x\": coords_all[:,0],\n",
    "    \"umap_y\": coords_all[:,1],\n",
    "})\n",
    "coords_all_df.to_parquet(run_stem(\"umap_all_models_coords.parquet\"), index=False)\n",
    "coords_all_df.to_csv(run_stem(\"umap_all_models_coords.csv\"), index=False)\n",
    "print(\"Saved base UMAP coords to:\", run_stem(\"umap_all_models_coords.parquet\"), \"and\", run_stem(\"umap_all_models_coords.csv\"))\n",
    "print(\"Ethical Note: UMAP is for visual inspection only and not a clustering tool.\")\n",
    "toc(t0, \"UMAP base ready\")\n",
    "\n",
    "# 5) Plot helper (centroids computed from plotted subset only)\n",
    "def _plot_clusters(coords2d, labels, title, out_png, max_points=PLOT_MAX):\n",
    "    n = coords2d.shape[0]\n",
    "    if n > max_points:\n",
    "        idx = RNG.choice(n, size=max_points, replace=False)\n",
    "        pts, labs = coords2d[idx], labels[idx]\n",
    "    else:\n",
    "        pts, labs = coords2d, labels\n",
    "\n",
    "    # color mapping: noise in light gray\n",
    "    base_colors = plt.rcParams['axes.prop_cycle'].by_key().get('color', ['C0','C1','C2','C3','C4','C5','C6','C7','C8','C9'])\n",
    "    uniq = np.unique(labs)\n",
    "    uniq_sorted = [u for u in uniq if u != -1] + ([-1] if -1 in uniq else [])\n",
    "    color_map, c_idx = {}, 0\n",
    "    for u in uniq_sorted:\n",
    "        if u == -1:\n",
    "            color_map[u] = \"#D3D3D3\"\n",
    "        else:\n",
    "            color_map[u] = base_colors[c_idx % len(base_colors)]\n",
    "            c_idx += 1\n",
    "\n",
    "    plt.figure(figsize=(8,8), dpi=PLOT_DPI)\n",
    "    for u in uniq_sorted:\n",
    "        m = (labs == u)\n",
    "        if not np.any(m): \n",
    "            continue\n",
    "        plt.scatter(pts[m,0], pts[m,1], s=POINT_SIZE, alpha=0.55, linewidths=0,\n",
    "                    color=color_map[u], label=(\"Noise\" if u == -1 else f\"Cluster {u}\"), rasterized=True)\n",
    "\n",
    "    # centroids from the plotted subset\n",
    "    cents = []\n",
    "    for u in [u for u in uniq_sorted if u != -1]:\n",
    "        m = (labs == u)\n",
    "        if np.any(m):\n",
    "            cents.append(pts[m].mean(axis=0))\n",
    "    if cents:\n",
    "        cents = np.vstack(cents)\n",
    "        plt.scatter(cents[:,0], cents[:,1], s=120, marker=\"X\", edgecolor=\"black\",\n",
    "                    linewidths=0.5, color=[color_map[u] for u in uniq_sorted if u != -1], label=\"Centroids\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"UMAP-1\"); plt.ylabel(\"UMAP-2\")\n",
    "    plt.legend(markerscale=4, fontsize=8, frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=PLOT_DPI, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved:\", out_png)\n",
    "\n",
    "# 6) Label alignment helper\n",
    "def _align_labels(name, arr):\n",
    "    \"\"\"Ensures that labels align with BASE_X row order (SESSION_INDEX). Returns None if not alignable.\"\"\"\n",
    "    if len(arr) == len(SESSION_INDEX):\n",
    "        return np.asarray(arr)\n",
    "    if (\"rfm_proxy\" in globals()) and isinstance(rfm_proxy, pd.DataFrame):\n",
    "        if len(arr) == rfm_proxy.shape[0]:\n",
    "            lab_series = pd.Series(arr, index=rfm_proxy.index)\n",
    "            aligned = lab_series.reindex(SESSION_INDEX).to_numpy()\n",
    "            return aligned if aligned.shape[0] == len(SESSION_INDEX) else None\n",
    "        if name == \"rfm_proxy\" and \"rfm_proxy_label\" in rfm_proxy.columns:\n",
    "            aligned = rfm_proxy[\"rfm_proxy_label\"].reindex(SESSION_INDEX).to_numpy()\n",
    "            return aligned if aligned.shape[0] == len(SESSION_INDEX) else None\n",
    "    return None\n",
    "\n",
    "# 7) Plot overlays for ALL models on the neural embedding UMAP\n",
    "t0 = tic(\"Plotting neural-embedding overlays for all models\")\n",
    "all_plots = []  # (title, filename)\n",
    "\n",
    "for key, lab in labels_dict.items():\n",
    "    if key.endswith(\"_conf\"):  # skip HDBSCAN strengths\n",
    "        continue\n",
    "    labels_aligned = _align_labels(key, lab)\n",
    "    if labels_aligned is None:\n",
    "        print(f\"[skip] {key} — label array length mismatch (expected {len(SESSION_INDEX)})\")\n",
    "        continue\n",
    "\n",
    "    pretty = (key\n",
    "              .replace(\"_knnprop_conf\",\"\")\n",
    "              .replace(\"_knnprop\",\"\")\n",
    "              .replace(\"agg_\",\"Agglomerative \")\n",
    "              .replace(\"kmeans_\",\"KMeans k=\")\n",
    "              .replace(\"gmm_\",\"GMM k=\")\n",
    "              .replace(\"dbscan_\",\"DBSCAN \")\n",
    "              .replace(\"hdbscan\",\"HDBSCAN\"))\n",
    "    fname = f\"{RUN_ID}_umap_{key}.png\".replace(\"__\",\"_\")\n",
    "    out_png = FIGS_DIR / fname\n",
    "    _plot_clusters(coords_all, labels_aligned, f\"Neural-embedding UMAP — {pretty}\", out_png)\n",
    "    all_plots.append((pretty, out_png))\n",
    "\n",
    "toc(t0, \"neural overlays completed\")\n",
    "\n",
    "# 8) Separate UMAP for RFM baseline (3 features)\n",
    "if (\"X_rfm\" in globals()) and (\"rfm_proxy\" in globals()):\n",
    "    t0 = tic(\"UMAP for session RFM-proxy (3 features): sample-fit + transform\")\n",
    "    X_rfm = X_rfm.astype(np.float32, copy=False)\n",
    "    n_rfm  = X_rfm.shape[0]\n",
    "    fit_n  = min(60_000, n_rfm)\n",
    "    fit_idx = RNG.choice(n_rfm, size=fit_n, replace=False)\n",
    "\n",
    "    umap_rfm = umap.UMAP(\n",
    "        n_neighbors=UMAP_N_NEIGHBORS,\n",
    "        min_dist=UMAP_MIN_DIST,\n",
    "        n_components=2,\n",
    "        random_state=RANDOM_SEED,\n",
    "        metric=\"euclidean\",\n",
    "        low_memory=LOW_MEMORY,\n",
    "    ).fit(X_rfm[fit_idx])\n",
    "\n",
    "    coords_rfm = np.empty((n_rfm, 2), dtype=np.float32)\n",
    "    coords_rfm[fit_idx] = umap_rfm.embedding_.astype(np.float32, copy=False)\n",
    "    rest = np.setdiff1d(np.arange(n_rfm), fit_idx, assume_unique=True)\n",
    "    for s in range(0, len(rest), BATCH_SIZE):\n",
    "        b = rest[s:s+BATCH_SIZE]\n",
    "        coords_rfm[b] = umap_rfm.transform(X_rfm[b]).astype(np.float32, copy=False)\n",
    "\n",
    "    # plot any RFM labels present\n",
    "    for key, lab in labels_dict.items():\n",
    "        if key.startswith(\"rfm_proxy_\") and (not key.endswith(\"_conf\")):\n",
    "            labels_rfm = _align_labels(\"rfm_proxy\", lab)\n",
    "            if labels_rfm is None:\n",
    "                print(f\"[skip RFM plot] {key} — label array length mismatch (expected {len(SESSION_INDEX)})\")\n",
    "                continue\n",
    "            out_png = FIGS_DIR / f\"{RUN_ID}_umap_{key}.png\"\n",
    "            _plot_clusters(coords_rfm, labels_rfm, f\"RFM-proxy UMAP — {key}\", out_png)\n",
    "            all_plots.append((key, out_png))\n",
    "\n",
    "    # saving RFM coordinates (run-stamped)\n",
    "    coords_rfm_df = pd.DataFrame({\n",
    "        \"session_id\": rfm_proxy.index.values,\n",
    "        \"umap_x\": coords_rfm[:,0],\n",
    "        \"umap_y\": coords_rfm[:,1],\n",
    "    })\n",
    "    coords_rfm_df.to_parquet(run_stem(\"umap_rfm_proxy_coords.parquet\"), index=False)\n",
    "    coords_rfm_df.to_csv(run_stem(\"umap_rfm_proxy_coords.csv\"), index=False)\n",
    "    toc(t0, \"RFM-proxy UMAP completed\")\n",
    "else:\n",
    "    print(\"Skipped RFM-proxy UMAP: X_rfm/rfm_proxy not found (run 5.1 & 5.2 first).\")\n",
    "\n",
    "print(\"\\nGenerated plots:\")\n",
    "for ttitle, p in all_plots:\n",
    "    print(\"-\", ttitle, \"->\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dee2c1",
   "metadata": {},
   "source": [
    "## 7. Model evaluation (internal metrics, cross-model agreement and temporal drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce6e9e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Starting: Evaluation — prechecks\n",
      "Eval sample size: 70000\n",
      "---- completed: Evaluation — prechecks completed in 0.00s\n",
      "\n",
      ">>> Starting: Internal metrics per model\n",
      "[DBCV skipped for kmeans_2] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for kmeans_3] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for kmeans_4] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for kmeans_6] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for kmeans_8] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for kmeans_10] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for kmeans_12] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for kmeans_14] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for gmm_3] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for gmm_4] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for gmm_6] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for gmm_8] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for gmm_10] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for gmm_12] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for gmm_14] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for gmm_16] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for hdbscan] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for agg_ward_12] validity_index() missing 1 required positional argument: 'labels'\n",
      "[DBCV skipped for rfm_proxy_kmeans_session_6] validity_index() missing 1 required positional argument: 'labels'\n",
      "Saved internal metrics to: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_eval_internal_metrics.csv\n",
      "                         model  n_clusters  silhouette  calinski_harabasz  \\\n",
      "19  rfm_proxy_kmeans_session_6           6    0.412091       64119.376256   \n",
      "0                     kmeans_2           2    0.123996        8090.697024   \n",
      "16                     hdbscan           2    0.123675         587.070133   \n",
      "17                 agg_ward_12          12    0.109715        1530.120129   \n",
      "6                    kmeans_12          12    0.078461        1690.674269   \n",
      "7                    kmeans_14          14    0.078288        1556.807878   \n",
      "1                     kmeans_3           3    0.074744        4822.685682   \n",
      "5                    kmeans_10          10    0.071976        1896.169348   \n",
      "2                     kmeans_4           4    0.069919        3821.706152   \n",
      "15                      gmm_16          16    0.069624         847.840960   \n",
      "\n",
      "    davies_bouldin  dbcv  \n",
      "19        0.802793   NaN  \n",
      "0         2.618591   NaN  \n",
      "16        1.859728   NaN  \n",
      "17        3.647710   NaN  \n",
      "6         3.790585   NaN  \n",
      "7         3.712835   NaN  \n",
      "1         4.735308   NaN  \n",
      "5         3.927402   NaN  \n",
      "2         4.483988   NaN  \n",
      "15        6.631089   NaN  \n",
      "---- completed: Internal metrics per model in 12.22s\n",
      "\n",
      ">>> Starting: Cross-model agreement (ARI / AMI)\n",
      "Saved ARI -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_eval_cross_model_ARI.csv\n",
      "Saved AMI -> C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_eval_cross_model_AMI.csv\n",
      "\n",
      "Top 5 ARI pairs:\n",
      "                        gmm_10  vs  gmm_14                          ->  ARI=0.854\n",
      "                        gmm_14  vs  gmm_16                          ->  ARI=0.786\n",
      "                         gmm_6  vs  gmm_8                           ->  ARI=0.776\n",
      "                        gmm_10  vs  gmm_12                          ->  ARI=0.731\n",
      "                        gmm_10  vs  gmm_8                           ->  ARI=0.724\n",
      "\n",
      "Bottom-5 ARI pairs:\n",
      "        dbscan_eps14.11645_ms8  vs  kmeans_4                        ->  ARI=-0.000\n",
      "        dbscan_eps14.11645_ms8  vs  kmeans_3                        ->  ARI=-0.000\n",
      "        dbscan_eps14.11645_ms8  vs  kmeans_2                        ->  ARI=-0.001\n",
      "        dbscan_eps14.11645_ms8  vs  hdbscan                         ->  ARI=-0.001\n",
      "                      kmeans_2  vs  rfm_proxy_kmeans_session_6      ->  ARI=-0.010\n",
      "---- completed: Cross-model agreement (ARI / AMI) in 103.79s\n",
      "\n",
      ">>> Starting: Temporal drift check (early vs late halves)\n",
      "Saved temporal drift summary to: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_eval_temporal_drift.csv\n",
      "                          model  cluster   p_early    p_late  abs_diff\n",
      "150  rfm_proxy_kmeans_session_6        3  0.000000  0.780584  0.780584\n",
      "148  rfm_proxy_kmeans_session_6        1  0.772267  0.014115  0.758152\n",
      "4                      kmeans_3        2  0.269131  0.229983  0.039148\n",
      "14                     kmeans_6        5  0.122594  0.084886  0.037708\n",
      "21                     kmeans_8        6  0.111787  0.075538  0.036249\n",
      "68                        gmm_6        2  0.269115  0.300340  0.031225\n",
      "2                      kmeans_3        0  0.444912  0.474858  0.029946\n",
      "5                      kmeans_4        0  0.337419  0.308314  0.029105\n",
      "147  rfm_proxy_kmeans_session_6        0  0.125746  0.099314  0.026432\n",
      "27                    kmeans_10        4  0.051527  0.026418  0.025109\n",
      "---- completed: Temporal drift check in 0.98s\n",
      "time: 1min 57s (started: 2025-08-27 18:59:50 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# 7) Model evaluation (internal metrics, cross-model agreement, temporal drift)\n",
    "\n",
    "\n",
    "# 1) Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, calinski_harabasz_score, davies_bouldin_score,\n",
    "    adjusted_rand_score, adjusted_mutual_info_score\n",
    ")\n",
    "\n",
    "# 2) small timers\n",
    "def tic(section):\n",
    "    print(f\"\\n>>> Starting: {section}\")\n",
    "    return time.perf_counter()\n",
    "\n",
    "def toc(t0, section):\n",
    "    dt = time.perf_counter() - t0\n",
    "    print(f\"---- completed: {section} in {dt:.2f}s\")\n",
    "\n",
    "# 3) Pre-checks & pulling through config\n",
    "t0 = tic(\"Evaluation — prechecks\")\n",
    "assert \"labels_dict\" in globals() and len(labels_dict) > 0, \"labels_dict not found; run clustering first.\"\n",
    "\n",
    "RANDOM_SEED        = globals().get(\"RANDOM_SEED\", 42)\n",
    "RNG                = globals().get(\"RNG\", np.random.default_rng(RANDOM_SEED))\n",
    "EVAL_SAMPLE_SIZE   = int(globals().get(\"EVAL_SAMPLE_SIZE\", 10_000))\n",
    "COMPUTE_SILHOUETTE = bool(globals().get(\"COMPUTE_SILHOUETTE\", True))\n",
    "COMPUTE_CH_DB      = bool(globals().get(\"COMPUTE_CH_DB\", True))\n",
    "COMPUTE_DBCV       = bool(globals().get(\"COMPUTE_DBCV\", False))\n",
    "NOISE_LABEL        = int(globals().get(\"NOISE_LABEL\", -1))\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reuse existing S_EVAL_IDX if present; else deterministic sample\n",
    "if \"S_EVAL_IDX\" not in globals():\n",
    "    N_ref = next(v.shape[0] for k, v in labels_dict.items() if not k.endswith(\"_conf\"))\n",
    "    S_EVAL_IDX = RNG.choice(N_ref, size=min(EVAL_SAMPLE_SIZE, N_ref), replace=False)\n",
    "print(f\"Eval sample size: {len(S_EVAL_IDX)}\")\n",
    "toc(t0, \"Evaluation — prechecks completed\")\n",
    "\n",
    "# 4) Choose the feature space matching each model’s labels\n",
    "def _X_for_label(key: str):\n",
    "    \"\"\"\n",
    "    Return the feature matrix that matches how 'key' labels were produced.\n",
    "    Falls back to X_model_kmeans (L2-PCA) if ambiguous.\n",
    "    \"\"\"\n",
    "    if key.startswith(\"kmeans_\") or key.startswith(\"agg_\"):\n",
    "        return X_model_kmeans\n",
    "    if key.startswith(\"gmm_\"):\n",
    "        return X_model_gmm\n",
    "    if key.startswith(\"hdbscan_\"):\n",
    "        return X_model_hdbscan\n",
    "    if key.startswith(\"dbscan_\"):\n",
    "        return X_model_dbscan\n",
    "    if key.startswith(\"rfm_simple\") or key.startswith(\"rfm_proxy\"):\n",
    "        return globals().get(\"X_rfm\", X_model_kmeans)\n",
    "    return X_model_kmeans\n",
    "\n",
    "# 5) Internal metrics per model (silhouette / CH / DB / optional DBCV)\n",
    "t0 = tic(\"Internal metrics per model\")\n",
    "\n",
    "rows = []\n",
    "for name, y_full in labels_dict.items():\n",
    "    if name.endswith(\"_conf\"):\n",
    "        continue\n",
    "\n",
    "    X = _X_for_label(name)\n",
    "    idx = S_EVAL_IDX if len(S_EVAL_IDX) < X.shape[0] else np.arange(X.shape[0])\n",
    "\n",
    "    y = y_full[idx]\n",
    "    # ignore noise for geometry metrics\n",
    "    mask = (y != NOISE_LABEL) if (NOISE_LABEL in y) else np.ones_like(y, dtype=bool)\n",
    "    Xs, ys = X[idx][mask], y[mask]\n",
    "\n",
    "    sil = ch = db = np.nan\n",
    "    dbcv = np.nan\n",
    "\n",
    "    # need at least 2 clusters & enough points\n",
    "    if (np.unique(ys).size >= 2) and (Xs.shape[0] >= 100):\n",
    "        # stratified subsample for silhouette (keep ≥2 labels)\n",
    "        Xsub, ysub = Xs, ys\n",
    "        sub_n = min(5_000, Xs.shape[0])\n",
    "        if Xs.shape[0] > sub_n:\n",
    "            labels_u = np.unique(ys)\n",
    "            per = max(1, int(np.ceil(sub_n / labels_u.size)))\n",
    "            take_idx = []\n",
    "            for c in labels_u:\n",
    "                idx_c = np.where(ys == c)[0]\n",
    "                if idx_c.size > 0:\n",
    "                    k = min(per, idx_c.size)\n",
    "                    take_idx.append(RNG.choice(idx_c, size=k, replace=False))\n",
    "            take_idx = np.concatenate(take_idx)\n",
    "            if np.unique(ys[take_idx]).size >= 2:\n",
    "                Xsub, ysub = Xs[take_idx], ys[take_idx]\n",
    "\n",
    "        if COMPUTE_SILHOUETTE:\n",
    "            sil = float(silhouette_score(Xsub, ysub, metric=\"euclidean\"))\n",
    "        if COMPUTE_CH_DB:\n",
    "            ch  = float(calinski_harabasz_score(Xs, ys))\n",
    "            db  = float(davies_bouldin_score(Xs, ys))\n",
    "\n",
    "        # Optional: DBCV (density-based validity), best for density models\n",
    "        if COMPUTE_DBCV:\n",
    "            try:\n",
    "                from hdbscan.validity import validity_index\n",
    "                # Use cosine-like geometry when X is L2-normalised (kmeans/dbscan/hdbscan on L2-PCA)\n",
    "                # validity_index expects condensed distances or a metric name supported by pairwise distances.\n",
    "                # We’ll use 'euclidean' here because BASE_X is L2-normalised for cosine-like setups.\n",
    "                dbcv = float(validity_index(Xs, y=ys, metric='euclidean'))\n",
    "            except Exception as e:\n",
    "                # keep robust; just log once per model\n",
    "                print(f\"[DBCV skipped for {name}] {e}\")\n",
    "\n",
    "    # count clusters excl. noise\n",
    "    n_clusters = int(np.unique(y_full[y_full != NOISE_LABEL]).size) if (NOISE_LABEL in y_full) else int(np.unique(y_full).size)\n",
    "    rows.append({\n",
    "        \"model\": name, \"n_clusters\": n_clusters,\n",
    "        \"silhouette\": sil, \"calinski_harabasz\": ch, \"davies_bouldin\": db,\n",
    "        \"dbcv\": dbcv\n",
    "    })\n",
    "\n",
    "internal_df = pd.DataFrame(rows).sort_values([\"silhouette\"], ascending=False)\n",
    "internal_path = run_stem(\"eval_internal_metrics.csv\")\n",
    "internal_df.to_csv(internal_path, index=False)\n",
    "print(\"Saved internal metrics to:\", internal_path)\n",
    "print(internal_df.head(10))\n",
    "toc(t0, \"Internal metrics per model\")\n",
    "\n",
    "# 6) Cross-model agreement (ARI / AMI)\n",
    "t0 = tic(\"Cross-model agreement (ARI / AMI)\")\n",
    "\n",
    "keys = [k for k in labels_dict.keys() if not k.endswith(\"_conf\")]\n",
    "Y = {k: labels_dict[k] for k in keys}\n",
    "\n",
    "ari = pd.DataFrame(index=keys, columns=keys, dtype=\"float32\")\n",
    "ami = pd.DataFrame(index=keys, columns=keys, dtype=\"float32\")\n",
    "\n",
    "for ki in keys:\n",
    "    yi = Y[ki]\n",
    "    for kj in keys:\n",
    "        yj = Y[kj]\n",
    "        if len(yi) == len(yj):\n",
    "            ari.loc[ki, kj] = adjusted_rand_score(yi, yj)\n",
    "            ami.loc[ki, kj] = adjusted_mutual_info_score(yi, yj)\n",
    "        else:\n",
    "            ari.loc[ki, kj] = np.nan\n",
    "            ami.loc[ki, kj] = np.nan\n",
    "\n",
    "ari_path = run_stem(\"eval_cross_model_ARI.csv\")\n",
    "ami_path = run_stem(\"eval_cross_model_AMI.csv\")\n",
    "ari.to_csv(ari_path); ami.to_csv(ami_path)\n",
    "print(\"Saved ARI ->\", ari_path)\n",
    "print(\"Saved AMI ->\", ami_path)\n",
    "\n",
    "def _top_pairs(M, k=5):\n",
    "    vals = []\n",
    "    for i in M.index:\n",
    "        for j in M.columns:\n",
    "            if i >= j:  # avoid dup/self (matrix is symmetric)\n",
    "                continue\n",
    "            v = M.loc[i, j]\n",
    "            if pd.notnull(v):\n",
    "                vals.append((i, j, float(v)))\n",
    "    vals.sort(key=lambda x: x[2], reverse=True)\n",
    "    return vals[:k], vals[-k:]\n",
    "\n",
    "top5, bot5 = _top_pairs(ari)\n",
    "print(\"\\nTop 5 ARI pairs:\")\n",
    "for a,b,v in top5:  print(f\"{a:>30s}  vs  {b:<30s}  ->  ARI={v:.3f}\")\n",
    "print(\"\\nBottom-5 ARI pairs:\")\n",
    "for a,b,v in bot5:  print(f\"{a:>30s}  vs  {b:<30s}  ->  ARI={v:.3f}\")\n",
    "\n",
    "toc(t0, \"Cross-model agreement (ARI / AMI)\")\n",
    "\n",
    "# 7) Temporal drift (early vs late halves by session start time)\n",
    "t0 = tic(\"Temporal drift check (early vs late halves)\")\n",
    "\n",
    "# Build/confirm SESSION_INDEX\n",
    "if \"SESSION_INDEX\" not in globals():\n",
    "    if \"session_to_items\" in globals():\n",
    "        SESSION_INDEX = session_to_items.index.to_numpy()\n",
    "    else:\n",
    "        SESSION_INDEX = (events[[\"session_id\"]].drop_duplicates()\n",
    "                         .astype({\"session_id\":\"int64\"}).values.ravel())\n",
    "\n",
    "# Respect TS_DIV from config\n",
    "TS_DIV = int(globals().get(\"TS_DIV\", 1))\n",
    "sess_first_ts = (events.groupby(\"session_id\")[\"timestamp\"].min() // TS_DIV)\n",
    "sess_first_ts = sess_first_ts.reindex(SESSION_INDEX).to_numpy()\n",
    "median_ts = np.nanmedian(sess_first_ts)\n",
    "early = (sess_first_ts <= median_ts)\n",
    "late  = ~early\n",
    "\n",
    "drift_rows = []\n",
    "for name, y in Y.items():\n",
    "    labs = np.asarray(y)\n",
    "    uniq = np.unique(labs)\n",
    "    for c in uniq:\n",
    "        if c == NOISE_LABEL:  # skip noise here\n",
    "            continue\n",
    "        p_early = float((labs[early] == c).mean())\n",
    "        p_late  = float((labs[late]  == c).mean())\n",
    "        drift_rows.append({\n",
    "            \"model\": name, \"cluster\": int(c),\n",
    "            \"p_early\": p_early, \"p_late\": p_late,\n",
    "            \"abs_diff\": abs(p_early - p_late)\n",
    "        })\n",
    "\n",
    "drift_df = pd.DataFrame(drift_rows).sort_values([\"abs_diff\"], ascending=False)\n",
    "TEMPORAL_OUT = run_stem(\"eval_temporal_drift.csv\")\n",
    "drift_df.to_csv(TEMPORAL_OUT, index=False)\n",
    "print(\"Saved temporal drift summary to:\", TEMPORAL_OUT)\n",
    "print(drift_df.head(10))\n",
    "toc(t0, \"Temporal drift check\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c0594",
   "metadata": {},
   "source": [
    "## 8. Explainability via a surrogate RandomForest + SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04d00d55",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHAP] Lead model: kmeans_2\n",
      "[SHAP] Trained & saved surrogate RF.\n",
      "[SHAP] Sampled for SHAP: 30000 rows across 2 clusters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 3994/4000 [09:14<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHAP] Saved: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_shap_global_kmeans_2.csv and C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_shap_top_features_per_cluster_kmeans_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 5994/6000 [16:27<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHAP] Saved beeswarm: C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\20250827_144050_shap_beeswarm_kmeans_2.png\n",
      "Note: SHAP indicates feature association with cluster assignments, not causality. Correlated features may share credit.\n",
      "time: 1h 56min 38s (started: 2025-08-27 19:01:47 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# 7) Surrogate explainability with SHAP\n",
    "\n",
    "# 1) Imports\n",
    "import os, gc, joblib, numpy as np, pandas as pd, shap\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 2) pulling through from config (defaults safe if missing)\n",
    "USE_SHAP_SURROGATE   = bool(globals().get(\"USE_SHAP_SURROGATE\", True))\n",
    "if not USE_SHAP_SURROGATE:\n",
    "    print(\"[SHAP] USE_SHAP_SURROGATE=False, skipping this section.\")\n",
    "else:\n",
    "    RANDOM_SEED         = int(globals().get(\"RANDOM_SEED\", 42))\n",
    "    RNG                 = globals().get(\"RNG\", np.random.default_rng(RANDOM_SEED))\n",
    "    N_JOBS              = int(globals().get(\"N_JOBS\", -1))\n",
    "    NOISE_LABEL         = int(globals().get(\"NOISE_LABEL\", -1))\n",
    "\n",
    "    OUT_DIR             = globals().get(\"OUT_DIR\", Path(\"outputs\")); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    SHAP_DIR            = globals().get(\"SHAP_DIR\", OUT_DIR / \"shap_artifacts\"); SHAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    RF_N_ESTIMATORS     = int(globals().get(\"RF_N_ESTIMATORS\", 400))\n",
    "    RF_MAX_DEPTH        = globals().get(\"RF_MAX_DEPTH\", None)\n",
    "    RF_RANDOM_STATE     = RANDOM_SEED\n",
    "    SHAP_SAMPLE_SIZE    = int(globals().get(\"SHAP_SAMPLE_SIZE\", 100_000))     # total cap across clusters\n",
    "    SHAP_TOP_N_FEATURES = int(globals().get(\"SHAP_TOP_N_FEATURES\", 15))\n",
    "    SHAP_BACKGROUND_SIZE= int(globals().get(\"SHAP_BACKGROUND_SIZE\", 2_000))\n",
    "    SAVE_BEESWARM_PNG   = bool(globals().get(\"SAVE_BEESWARM_PNG\", True))\n",
    "\n",
    "    assert \"labels_dict\" in globals() and labels_dict, \"labels_dict is missing. Run clustering first.\"\n",
    "\n",
    "    # 3) Pick the lead clustering model (prefer neural over RFM unless LEAD_KEY provided)\n",
    "    lead_key = globals().get(\"LEAD_KEY\", None)\n",
    "\n",
    "    def _pick_lead_key():\n",
    "        if \"internal_df\" in globals() and isinstance(internal_df, pd.DataFrame) and not internal_df.empty:\n",
    "            order = internal_df.sort_values(\"silhouette\", ascending=False)[\"model\"].tolist()\n",
    "            for m in order:\n",
    "                if not m.startswith(\"rfm_proxy_\"):\n",
    "                    return m\n",
    "            return order[0]\n",
    "        path = OUT_DIR / \"eval_internal_metrics.csv\"\n",
    "        if path.exists():\n",
    "            df = pd.read_csv(path)\n",
    "            order = df.sort_values(\"silhouette\", ascending=False)[\"model\"].tolist()\n",
    "            for m in order:\n",
    "                if not m.startswith(\"rfm_proxy_\"):\n",
    "                    return m\n",
    "            return order[0]\n",
    "        # fallback: any kmeans_* else first non-conf entry\n",
    "        return next((k for k in labels_dict if k.startswith(\"kmeans_\") and not k.endswith(\"_conf\")),\n",
    "                    next(k for k in labels_dict if not k.endswith(\"_conf\")))\n",
    "\n",
    "    if lead_key is None:\n",
    "        lead_key = _pick_lead_key()\n",
    "\n",
    "    print(f\"[SHAP] Lead model: {lead_key}\")\n",
    "    y_all = np.asarray(labels_dict[lead_key])\n",
    "\n",
    "    # 4) Feature table for surrogate (use 'out' if present, else 'sess')\n",
    "    feat_df = out.copy() if \"out\" in globals() else sess.copy()\n",
    "\n",
    "    # Align row order to SESSION_INDEX (same as clustering inputs)\n",
    "    if \"SESSION_INDEX\" in globals():\n",
    "        if \"session_id\" in feat_df.columns:\n",
    "            feat_df = feat_df.set_index(\"session_id\")\n",
    "        feat_df = feat_df.reindex(pd.Index(SESSION_INDEX))\n",
    "\n",
    "    # Drop non-numeric + housekeeping columns, clean NaN/inf, remove constant cols\n",
    "    drop_cols = [c for c in feat_df.columns if c.startswith(\"rfm_proxy_\")] + [\"session_id\"]\n",
    "    feat_df = (feat_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "                     .select_dtypes(include=[np.number])\n",
    "                     .replace([np.inf, -np.inf], np.nan)\n",
    "                     .fillna(0.0))\n",
    "    feat_df = feat_df.loc[:, feat_df.nunique(dropna=False) > 1]\n",
    "\n",
    "    X_full = feat_df.to_numpy().astype(\"float32\", copy=False)\n",
    "    y_full = y_all\n",
    "\n",
    "    # Remove noise labels from training\n",
    "    mask_train = (y_full != NOISE_LABEL)\n",
    "    X_train, y_train = X_full[mask_train], y_full[mask_train]\n",
    "    assert X_train.shape[0] > 0, \"No non-noise labels available for SHAP surrogate.\"\n",
    "\n",
    "    # 5) Train/load RandomForest surrogate (fully configurable)\n",
    "    rf_path = SHAP_DIR / f\"rf_surrogate_{lead_key}.joblib\"\n",
    "    if rf_path.exists():\n",
    "        rf = joblib.load(rf_path)\n",
    "        print(\"[SHAP] Loaded surrogate RF from disk.\")\n",
    "    else:\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=RF_N_ESTIMATORS,\n",
    "            max_depth=RF_MAX_DEPTH,\n",
    "            max_features=\"sqrt\",\n",
    "            min_samples_leaf=20,\n",
    "            n_jobs=N_JOBS if isinstance(N_JOBS, int) else -1,\n",
    "            random_state=RF_RANDOM_STATE\n",
    "        ).fit(X_train, y_train)\n",
    "        joblib.dump(rf, rf_path)\n",
    "        print(\"[SHAP] Trained & saved surrogate RF.\")\n",
    "\n",
    "    # 6) Balanced, capped SHAP sample across clusters (total cap = SHAP_SAMPLE_SIZE)\n",
    "    classes = np.sort(np.unique(y_train))\n",
    "    K = len(classes)\n",
    "    per_cls = max(500, int(np.ceil(SHAP_SAMPLE_SIZE / max(1, K))))\n",
    "    take_idx = []\n",
    "    for c in classes:\n",
    "        idx_c = np.where(y_train == c)[0]\n",
    "        if idx_c.size == 0:\n",
    "            continue\n",
    "        take = min(per_cls, idx_c.size)\n",
    "        take_idx.append(RNG.choice(idx_c, size=take, replace=False))\n",
    "    take_idx = np.concatenate(take_idx) if take_idx else np.arange(min(10_000, X_train.shape[0]))\n",
    "    X_shap, y_shap = X_train[take_idx], y_train[take_idx]\n",
    "    print(f\"[SHAP] Sampled for SHAP: {X_shap.shape[0]} rows across {K} clusters.\")\n",
    "    pd.Series(y_shap).value_counts().sort_index().to_csv(SHAP_DIR / f\"shap_sample_class_sizes_{lead_key}.csv\")\n",
    "\n",
    "    # 7) Background for TreeExplainer (interventional): small, stratified\n",
    "    bg_per_cls = max(50, int(np.ceil(SHAP_BACKGROUND_SIZE / max(1, K))))\n",
    "    bg_idx = []\n",
    "    for c in classes:\n",
    "        idx_c = np.where(y_train == c)[0]\n",
    "        if idx_c.size == 0:\n",
    "            continue\n",
    "        take = min(bg_per_cls, idx_c.size)\n",
    "        bg_idx.append(RNG.choice(idx_c, size=take, replace=False))\n",
    "    bg_idx = np.concatenate(bg_idx) if bg_idx else RNG.choice(X_train.shape[0], size=min(SHAP_BACKGROUND_SIZE, X_train.shape[0]), replace=False)\n",
    "    X_bg = X_train[bg_idx]\n",
    "\n",
    "    # Helper to normalise SHAP outputs across versions\n",
    "    def _as_class_list(sv):\n",
    "        if isinstance(sv, list):\n",
    "            return sv\n",
    "        if isinstance(sv, np.ndarray):\n",
    "            if sv.ndim == 3:               # (n,F,C)\n",
    "                return [sv[:, :, i] for i in range(sv.shape[2])]\n",
    "            if sv.ndim == 2:               # (n,F)\n",
    "                return [sv]\n",
    "        raise ValueError(f\"Unexpected SHAP output shape/type: {type(sv)}, ndim={getattr(sv,'ndim',None)}\")\n",
    "\n",
    "    # 8) Streaming SHAP aggregation (memory friendly)\n",
    "    explainer = shap.TreeExplainer(rf, data=X_bg, feature_perturbation=\"interventional\", model_output=\"raw\")\n",
    "    BATCH = 2000\n",
    "    F     = X_shap.shape[1]\n",
    "    C     = len(rf.classes_)\n",
    "    sum_abs_global     = np.zeros(F, dtype=np.float64)\n",
    "    count_global       = 0\n",
    "    sum_abs_by_cluster = np.zeros((C, F), dtype=np.float64)\n",
    "    count_by_cluster   = np.zeros(C, dtype=np.int64)\n",
    "\n",
    "    for s in range(0, X_shap.shape[0], BATCH):\n",
    "        xb = X_shap[s:s+BATCH]; yb = y_shap[s:s+BATCH]\n",
    "        raw = explainer.shap_values(xb, check_additivity=False)\n",
    "        clist = _as_class_list(raw)  # list of (n_batch,F)\n",
    "\n",
    "        # multiclass alignment; SHAP may return 1 array in binary case\n",
    "        if len(clist) == 1 and C == 2:\n",
    "            clist = [clist[0], clist[0]]\n",
    "\n",
    "        for ci, c in enumerate(rf.classes_):\n",
    "            sv_ci = clist[min(ci, len(clist)-1)]\n",
    "            m = (yb == c)\n",
    "            if not np.any(m):\n",
    "                continue\n",
    "            v = np.abs(sv_ci[m]).sum(axis=0)    # (F,)\n",
    "            sum_abs_by_cluster[ci] += v\n",
    "            count_by_cluster[ci]   += int(m.sum())\n",
    "            sum_abs_global         += v\n",
    "            count_global           += int(m.sum())\n",
    "\n",
    "        del raw, clist, xb, yb\n",
    "        if (s // BATCH) % 5 == 0:\n",
    "            np.savez_compressed(\n",
    "                SHAP_DIR / f\"shap_ckpt_{lead_key}.npz\",\n",
    "                sum_abs_global=sum_abs_global, count_global=np.array([count_global]),\n",
    "                sum_abs_by_cluster=sum_abs_by_cluster, count_by_cluster=count_by_cluster\n",
    "            )\n",
    "            gc.collect()\n",
    "\n",
    "    # 9) Save summaries (timestamped via run_stem if available)\n",
    "    feat_names = feat_df.columns.to_list()\n",
    "\n",
    "    def _out(path_like):\n",
    "        # use run_stem if available for consistent RUN_ID prefix\n",
    "        return run_stem(path_like) if \"run_stem\" in globals() else (OUT_DIR / path_like)\n",
    "\n",
    "    global_imp = pd.DataFrame({\n",
    "        \"feature\": feat_names,\n",
    "        \"mean_abs_shap\": sum_abs_global / max(1, count_global)\n",
    "    }).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "    global_path = _out(f\"shap_global_{lead_key}.csv\")\n",
    "    global_imp.to_csv(global_path, index=False)\n",
    "\n",
    "    per_cluster_rows = []\n",
    "    for ci, c in enumerate(rf.classes_):\n",
    "        if count_by_cluster[ci] == 0:\n",
    "            continue\n",
    "        tmp = pd.DataFrame({\n",
    "            \"cluster\": int(c),\n",
    "            \"feature\": feat_names,\n",
    "            \"mean_abs_shap\": (sum_abs_by_cluster[ci] / max(1, count_by_cluster[ci]))\n",
    "        }).sort_values(\"mean_abs_shap\", ascending=False).head(SHAP_TOP_N_FEATURES)\n",
    "        tmp[\"rank\"] = np.arange(1, len(tmp) + 1)\n",
    "        per_cluster_rows.append(tmp)\n",
    "\n",
    "    top_path = _out(f\"shap_top_features_per_cluster_{lead_key}.csv\")\n",
    "    if per_cluster_rows:\n",
    "        pd.concat(per_cluster_rows, ignore_index=True).to_csv(top_path, index=False)\n",
    "\n",
    "    # Optional extra export if SHAP_TABLE_OUT set in config\n",
    "    SHAP_TABLE_OUT = globals().get(\"SHAP_TABLE_OUT\", None)\n",
    "    if SHAP_TABLE_OUT is not None and per_cluster_rows:\n",
    "        pd.concat(per_cluster_rows, ignore_index=True).to_csv(SHAP_TABLE_OUT, index=False)\n",
    "\n",
    "    print(\"[SHAP] Saved:\", global_path, \"and\", top_path)\n",
    "\n",
    "    # 10) Tiny beeswarm (subsampled) if enabled\n",
    "    if SAVE_BEESWARM_PNG and X_shap.shape[0] > 0:\n",
    "        import matplotlib.pyplot as plt\n",
    "        idx = RNG.choice(X_shap.shape[0], size=min(3000, X_shap.shape[0]), replace=False)\n",
    "        raw   = explainer.shap_values(X_shap[idx], check_additivity=False)\n",
    "        clist = _as_class_list(raw)\n",
    "\n",
    "        plt.figure(figsize=(8,6), dpi=140)\n",
    "        if len(clist) > 1:  # multiclass or mirrored binary\n",
    "            counts = [np.sum(y_shap[idx] == c) for c in rf.classes_]\n",
    "            top_ci = int(np.argmax(counts))\n",
    "            shap.summary_plot(clist[min(top_ci, len(clist)-1)], X_shap[idx],\n",
    "                              feature_names=feat_names, show=False)\n",
    "        else:\n",
    "            shap.summary_plot(clist[0], X_shap[idx], feature_names=feat_names, show=False)\n",
    "        plt.tight_layout()\n",
    "        fig_path = _out(f\"shap_beeswarm_{lead_key}.png\")\n",
    "        plt.savefig(fig_path, dpi=140, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(\"[SHAP] Saved beeswarm:\", fig_path)\n",
    "\n",
    "    print(\"Ethical Note: SHAP indicates feature association with cluster assignments, not causality. Correlated features may share credit.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b0d2c-aba4-4634-ab63-8ba8ae63d165",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation & Identifying Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b37f5a4e-6bb7-43d6-8102-522cc251abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RFM profile (top rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>n</th>\n",
       "      <th>share</th>\n",
       "      <th>recency_days_median</th>\n",
       "      <th>frequency_median</th>\n",
       "      <th>monetary_median</th>\n",
       "      <th>events_mean</th>\n",
       "      <th>p_view_mean</th>\n",
       "      <th>p_addtocart_mean</th>\n",
       "      <th>p_transaction_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>145960</td>\n",
       "      <td>0.393322</td>\n",
       "      <td>0.105023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.417505</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>144785</td>\n",
       "      <td>0.390156</td>\n",
       "      <td>0.037303</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.440356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>41761</td>\n",
       "      <td>0.112535</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.975647</td>\n",
       "      <td>0.999360</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20639</td>\n",
       "      <td>0.055616</td>\n",
       "      <td>0.071343</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.490576</td>\n",
       "      <td>0.643163</td>\n",
       "      <td>0.356837</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>14920</td>\n",
       "      <td>0.040205</td>\n",
       "      <td>0.069450</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.335925</td>\n",
       "      <td>0.495478</td>\n",
       "      <td>0.314757</td>\n",
       "      <td>0.189765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3030</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.068519</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.258746</td>\n",
       "      <td>0.479642</td>\n",
       "      <td>0.332943</td>\n",
       "      <td>0.187415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label       n     share  recency_days_median  frequency_median  \\\n",
       "1      1  145960  0.393322             0.105023               2.0   \n",
       "3      3  144785  0.390156             0.037303               2.0   \n",
       "0      0   41761  0.112535             0.078600               6.0   \n",
       "4      4   20639  0.055616             0.071343               3.0   \n",
       "5      5   14920  0.040205             0.069450               4.0   \n",
       "2      2    3030  0.008165             0.068519              11.0   \n",
       "\n",
       "   monetary_median  events_mean  p_view_mean  p_addtocart_mean  \\\n",
       "1              0.0     2.417505     1.000000          0.000000   \n",
       "3              0.0     2.440356     1.000000          0.000000   \n",
       "0              0.0     7.975647     0.999360          0.000640   \n",
       "4              0.0     3.490576     0.643163          0.356837   \n",
       "5              1.0     5.335925     0.495478          0.314757   \n",
       "2              2.0    14.258746     0.479642          0.332943   \n",
       "\n",
       "   p_transaction_mean  \n",
       "1            0.000000  \n",
       "3            0.000000  \n",
       "0            0.000000  \n",
       "4            0.000000  \n",
       "5            0.189765  \n",
       "2            0.187415  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved profile table : C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\rfm_cluster_profile_session.csv\n",
      "Saved personas to   : C:\\Users\\Admin\\Documents\\WBS\\Dissertation\\Submission Related files\\Notebooks\\Modelling\\outputs\\personas.csv\n",
      "time: 3.83 s (started: 2025-08-27 20:58:25 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# 9). Model Evaluation & Identifying Personas\n",
    "\n",
    "# 1) Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# 2) Config & paths (pulled from global CONFIG block where possible)\n",
    "BASELINE_LEVEL = globals().get(\"BASELINE_LEVEL\", \"session\")        # \"session\" | \"visitor\"\n",
    "MONETARY_MODE  = globals().get(\"MONETARY_MODE\", \"transactions\")    # \"transactions\" | \"none\"\n",
    "OUT_DIR        = globals().get(\"OUT_DIR\", Path(\"outputs\")); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PERSONA_OUT    = globals().get(\"PERSONA_OUT\", OUT_DIR / \"personas.csv\")\n",
    "TS_DIV         = float(globals().get(\"TS_DIV\", 1))                 # respect timestamp units\n",
    "RANDOM_SEED    = globals().get(\"RANDOM_SEED\", 42)\n",
    "RNG            = globals().get(\"RNG\", np.random.default_rng(RANDOM_SEED))\n",
    "\n",
    "# 3) Preconditions\n",
    "assert \"events\" in globals() and isinstance(events, pd.DataFrame), \"Missing 'events' dataframe from earlier blocks.\"\n",
    "need_cols = {\"session_id\", \"visitorid\", \"timestamp\", \"event\"}\n",
    "missing   = need_cols - set(events.columns)\n",
    "assert not missing, f\"'events' is missing columns: {missing}\"\n",
    "\n",
    "# 4) Build unit table (R/F/M inputs)\n",
    "if BASELINE_LEVEL == \"session\":\n",
    "    unit_ids  = events[\"session_id\"].drop_duplicates().astype(\"int64\")\n",
    "    unit_tbl  = pd.DataFrame({\"unit_id\": unit_ids.values})\n",
    "    sess_last = (events.groupby(\"session_id\")[\"timestamp\"].max() // TS_DIV)\n",
    "    sess_freq = events.groupby(\"session_id\").size()\n",
    "    tx_counts = (events.assign(event=events[\"event\"].astype(str).str.lower())\n",
    "                      .loc[lambda d: d[\"event\"] == \"transaction\"]\n",
    "                      .groupby(\"session_id\").size())\n",
    "    unit_tbl = (unit_tbl\n",
    "        .merge(sess_last.rename(\"last_ts\"), left_on=\"unit_id\", right_index=True, how=\"left\")\n",
    "        .merge(sess_freq.rename(\"frequency\"), left_on=\"unit_id\", right_index=True, how=\"left\")\n",
    "        .merge(tx_counts.rename(\"tx_count\"), left_on=\"unit_id\", right_index=True, how=\"left\"))\n",
    "else:  # visitor-level\n",
    "    unit_ids = events[\"visitorid\"].drop_duplicates()\n",
    "    unit_tbl = pd.DataFrame({\"unit_id\": unit_ids.values})\n",
    "    vis_last = (events.groupby(\"visitorid\")[\"timestamp\"].max() // TS_DIV)\n",
    "    vis_freq = events.groupby(\"visitorid\").size()\n",
    "    tx_counts = (events.assign(event=events[\"event\"].astype(str).str.lower())\n",
    "                      .loc[lambda d: d[\"event\"] == \"transaction\"]\n",
    "                      .groupby(\"visitorid\").size())\n",
    "    unit_tbl = (unit_tbl\n",
    "        .merge(vis_last.rename(\"last_ts\"), left_on=\"unit_id\", right_index=True, how=\"left\")\n",
    "        .merge(vis_freq.rename(\"frequency\"), left_on=\"unit_id\", right_index=True, how=\"left\")\n",
    "        .merge(tx_counts.rename(\"tx_count\"), left_on=\"unit_id\", right_index=True, how=\"left\"))\n",
    "\n",
    "# 5) Recency (days) & monetary\n",
    "max_ts = int((events[\"timestamp\"].max() // TS_DIV))\n",
    "unit_tbl[\"recency_days\"] = ((max_ts - unit_tbl[\"last_ts\"]) / (60 * 60 * 24)).astype(\"float64\")\n",
    "unit_tbl[\"tx_count\"]     = unit_tbl[\"tx_count\"].fillna(0).astype(\"float64\")\n",
    "unit_tbl[\"monetary\"]     = (1.0 if MONETARY_MODE == \"none\" else unit_tbl[\"tx_count\"])\n",
    "\n",
    "# 6) Attach RFM cluster labels (prefer labels from 'out', else fall back to labels_dict)\n",
    "label_series = None\n",
    "\n",
    "if BASELINE_LEVEL == \"session\":\n",
    "    # try from 'out' (RFM KMeans block)\n",
    "    if \"out\" in globals() and isinstance(out, pd.DataFrame) and (\"rfm_proxy_label\" in out.columns):\n",
    "        lab_src = out[\"rfm_proxy_label\"]\n",
    "        if lab_src.index.name != \"session_id\":\n",
    "            if \"session_id\" in out.columns:\n",
    "                lab_src = pd.Series(lab_src.values, index=out[\"session_id\"].values)\n",
    "            elif \"SESSION_INDEX\" in globals():\n",
    "                lab_src = pd.Series(lab_src.values, index=pd.Index(SESSION_INDEX, name=\"session_id\"))\n",
    "        label_series = lab_src.rename(\"label\").astype(int, copy=False)\n",
    "\n",
    "    # fallback to labels_dict\n",
    "    if (label_series is None) and (\"labels_dict\" in globals()):\n",
    "        rfm_keys = [k for k in labels_dict if k.startswith(\"rfm_proxy_kmeans_session_\")]\n",
    "        if rfm_keys:\n",
    "            k0 = sorted(rfm_keys)[0]\n",
    "            if \"sess\" in globals():\n",
    "                idx = sess.index\n",
    "            elif \"SESSION_INDEX\" in globals():\n",
    "                idx = pd.Index(SESSION_INDEX, name=\"session_id\")\n",
    "            else:\n",
    "                idx = events[\"session_id\"].drop_duplicates().astype(\"int64\")\n",
    "            label_series = pd.Series(labels_dict[k0], index=idx, name=\"label\").astype(int, copy=False)\n",
    "\n",
    "    assert label_series is not None, \"RFM session labels not found. Run Section 5.2 first.\"\n",
    "    unit_lbl = unit_tbl.merge(label_series.rename_axis(\"session_id\"),\n",
    "                              left_on=\"unit_id\", right_index=True, how=\"left\")\n",
    "else:\n",
    "    # visitor-level: derive majority label across that visitor's sessions\n",
    "    if \"out\" in globals() and isinstance(out, pd.DataFrame) and (\"rfm_proxy_label\" in out.columns):\n",
    "        sess_lab = out[\"rfm_proxy_label\"]\n",
    "        if sess_lab.index.name != \"session_id\":\n",
    "            if \"session_id\" in out.columns:\n",
    "                sess_lab = pd.Series(sess_lab.values, index=out[\"session_id\"].values)\n",
    "            elif \"SESSION_INDEX\" in globals():\n",
    "                sess_lab = pd.Series(sess_lab.values, index=pd.Index(SESSION_INDEX, name=\"session_id\"))\n",
    "    else:\n",
    "        assert \"labels_dict\" in globals(), \"labels_dict not found; run the RFM KMeans block (5.2).\"\n",
    "        rfm_keys = [k for k in labels_dict if k.startswith(\"rfm_proxy_kmeans_session_\")]\n",
    "        assert rfm_keys, \"No RFM session labels found in labels_dict.\"\n",
    "        k0 = sorted(rfm_keys)[0]\n",
    "        if \"sess\" in globals():\n",
    "            idx = sess.index\n",
    "        elif \"SESSION_INDEX\" in globals():\n",
    "            idx = pd.Index(SESSION_INDEX, name=\"session_id\")\n",
    "        else:\n",
    "            idx = events[\"session_id\"].drop_duplicates().astype(\"int64\")\n",
    "        sess_lab = pd.Series(labels_dict[k0], index=idx, name=\"rfm_proxy_label\")\n",
    "\n",
    "    sess_to_vis = (events[[\"session_id\", \"visitorid\"]]\n",
    "                   .drop_duplicates()\n",
    "                   .set_index(\"session_id\")[\"visitorid\"])\n",
    "    tmp = pd.DataFrame({\n",
    "        \"visitorid\": sess_to_vis.reindex(sess_lab.index).values,\n",
    "        \"label\": sess_lab.values\n",
    "    }).dropna()\n",
    "    vis_lab = (tmp.groupby(\"visitorid\")[\"label\"]\n",
    "                  .agg(lambda s: s.value_counts().idxmax())\n",
    "                  .astype(int))\n",
    "    unit_lbl = unit_tbl.merge(vis_lab.rename_axis(\"unit_id\").rename(\"label\"),\n",
    "                              left_on=\"unit_id\", right_index=True, how=\"left\")\n",
    "\n",
    "# 7) Event shares (views / add-to-cart / transactions)\n",
    "evt = events[[\"session_id\", \"visitorid\", \"event\"]].copy()\n",
    "evt[\"event\"] = evt[\"event\"].astype(str).str.lower()\n",
    "evt[\"is_view\"]        = (evt[\"event\"] == \"view\").astype(int)\n",
    "evt[\"is_addtocart\"]   = (evt[\"event\"] == \"addtocart\").astype(int)\n",
    "evt[\"is_transaction\"] = (evt[\"event\"] == \"transaction\").astype(int)\n",
    "\n",
    "if BASELINE_LEVEL == \"session\":\n",
    "    e_agg = (evt.groupby(\"session_id\")[[\"is_view\",\"is_addtocart\",\"is_transaction\"]]\n",
    "                .sum().reset_index().rename(columns={\"session_id\":\"unit_id\"}))\n",
    "else:\n",
    "    e_agg = (evt.groupby(\"visitorid\")[[\"is_view\",\"is_addtocart\",\"is_transaction\"]]\n",
    "                .sum().reset_index().rename(columns={\"visitorid\":\"unit_id\"}))\n",
    "e_agg[\"events\"] = e_agg[[\"is_view\",\"is_addtocart\",\"is_transaction\"]].sum(axis=1)\n",
    "\n",
    "prof = unit_lbl.merge(e_agg, on=\"unit_id\", how=\"left\")\n",
    "\n",
    "# proportions (safe for zero denominators)\n",
    "denom = prof[\"events\"].replace(0, np.nan)\n",
    "prof[\"p_view\"]        = (prof[\"is_view\"]        / denom).fillna(0.0)\n",
    "prof[\"p_addtocart\"]   = (prof[\"is_addtocart\"]   / denom).fillna(0.0)\n",
    "prof[\"p_transaction\"] = (prof[\"is_transaction\"] / denom).fillna(0.0)\n",
    "\n",
    "# 8) Cluster-level summary\n",
    "summary = (prof\n",
    "    .groupby(\"label\", dropna=False)\n",
    "    .agg(\n",
    "        n=(\"unit_id\", \"count\"),\n",
    "        share=(\"unit_id\", lambda s: len(s) / max(1, len(prof))),   # added share for readability\n",
    "        recency_days_median=(\"recency_days\", \"median\"),\n",
    "        frequency_median=(\"frequency\", \"median\"),\n",
    "        monetary_median=(\"monetary\", \"median\"),\n",
    "        events_mean=(\"events\", \"mean\"),\n",
    "        p_view_mean=(\"p_view\", \"mean\"),\n",
    "        p_addtocart_mean=(\"p_addtocart\", \"mean\"),\n",
    "        p_transaction_mean=(\"p_transaction\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values([\"n\"], ascending=False)\n",
    ")\n",
    "\n",
    "# 9) Save profile & personas\n",
    "summary_path = OUT_DIR / f\"rfm_cluster_profile_{BASELINE_LEVEL}.csv\"\n",
    "summary.to_csv(summary_path, index=False)\n",
    "summary.to_csv(PERSONA_OUT, index=False)\n",
    "\n",
    "print(\"Baseline RFM profile (top rows):\")\n",
    "try:\n",
    "    display(summary.head(10))\n",
    "except Exception:\n",
    "    print(summary.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"Saved profile table : {summary_path}\")\n",
    "print(f\"Saved personas to   : {PERSONA_OUT}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (seg)",
   "language": "python",
   "name": "seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
